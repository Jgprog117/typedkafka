{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"typedkafka","text":"<p>A well-documented, fully type-hinted Kafka client for Python.</p> <p>Built on confluent-kafka for performance and reliability.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Full type hints and comprehensive docstrings on every public API</li> <li>JSON, string, and bytes message helpers</li> <li>Transaction support with context managers</li> <li>Async producer and consumer via <code>asyncio</code></li> <li>Retry utilities with exponential backoff</li> <li>Pluggable serializers (JSON, String, Avro/Schema Registry)</li> <li>Metrics collection and statistics tracking (<code>KafkaMetrics</code>, <code>KafkaStats</code>)</li> <li>Dead letter queue helper (<code>DeadLetterQueue</code>, <code>process_with_dlq</code>)</li> <li>Message headers support on <code>send()</code></li> <li>Testing utilities (MockProducer/MockConsumer/MockDeadLetterQueue) with full API parity</li> <li>Type-safe config builders with validation and security helpers (SASL, SSL)</li> <li>Admin client for topic management</li> <li>Batch polling and consumer offset management (seek, assign, position)</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install typedkafka\n\n# With Avro/Schema Registry support\npip install typedkafka[avro]\n</code></pre> <p>Requires Python 3.9+.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from typedkafka import KafkaProducer\n\nwith KafkaProducer({\"bootstrap.servers\": \"localhost:9092\"}) as producer:\n    producer.send(\"my-topic\", b\"Hello, Kafka!\")\n    producer.send_json(\"events\", {\"user_id\": 123, \"action\": \"click\"})\n    producer.flush()\n</code></pre> <pre><code>from typedkafka import KafkaConsumer\n\nwith KafkaConsumer({\"bootstrap.servers\": \"localhost:9092\", \"group.id\": \"my-group\"}) as consumer:\n    consumer.subscribe([\"my-topic\"])\n    for msg in consumer:\n        print(msg.value_as_json())\n        consumer.commit(msg)\n</code></pre> <p>See the Getting Started guide for more examples.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Enhanced exceptions: <code>KafkaErrorContext</code> dataclass for structured error metadata; new <code>ConfigurationError</code> and <code>TransactionError</code> exception classes</li> <li>Configuration presets: <code>ProducerConfig.high_throughput()</code> and <code>exactly_once()</code> class methods for common setups</li> <li>Environment config loading: <code>ProducerConfig.from_env()</code> and <code>ConsumerConfig.from_env()</code> for 12-factor app config</li> <li><code>enable_idempotence()</code> and <code>transactional_id()</code> methods on <code>ProducerConfig</code></li> <li>Cross-field configuration validation (e.g. idempotence requires <code>acks=all</code>)</li> <li>Generic consumer deserialization: <code>KafkaMessage.value_as(deserializer)</code> and <code>value_deserializer</code> parameter on <code>KafkaConsumer</code></li> <li>Async improvements: <code>AsyncKafkaProducer.send_string()</code>, <code>MessageBatch</code> class, <code>batch_consume()</code> async generator</li> <li>Testing enhancements: <code>fail_on_topics</code> on <code>MockProducer</code>, <code>message_count()</code>, <code>get_json_messages()</code>, <code>MockConsumer.add_string_message()</code></li> <li>Integration test infrastructure: <code>tests/integration/</code> with Docker Compose and CI workflow for Kafka broker tests</li> <li>Protobuf serialization: <code>ProtobufSerializer</code>, <code>ProtobufDeserializer</code>, <code>SchemaRegistryProtobufSerializer</code>, and helper functions in <code>typedkafka.protobuf</code></li> <li>OpenTelemetry tracing: <code>KafkaTracer</code> with <code>produce_span()</code> and <code>consume_span()</code> context managers following OTel semantic conventions; graceful no-op when OTel is not installed</li> <li><code>protobuf</code> and <code>all</code> optional dependency groups</li> <li>GitHub Pages docs deployment workflow</li> <li>Test coverage: 342 tests (up from 320)</li> </ul>"},{"location":"changelog/#050","title":"[0.5.0]","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Metrics collection: <code>KafkaMetrics</code> and <code>KafkaStats</code> dataclasses for tracking throughput, errors, and byte counters on producers and consumers</li> <li><code>on_stats</code> callback parameter on <code>KafkaProducer</code> and <code>KafkaConsumer</code> for receiving parsed statistics from confluent-kafka's <code>stats_cb</code></li> <li><code>stats_interval_ms()</code> method on <code>ProducerConfig</code> and <code>ConsumerConfig</code> builders</li> <li><code>metrics</code> property on <code>KafkaProducer</code>, <code>KafkaConsumer</code>, <code>MockProducer</code>, and <code>MockConsumer</code></li> <li>Dead Letter Queue: <code>DeadLetterQueue</code> class for routing failed messages to a DLQ topic with error metadata headers</li> <li><code>process_with_dlq()</code> helper for try/except message processing with automatic DLQ routing</li> <li><code>headers</code> parameter on <code>KafkaProducer.send()</code> and <code>MockProducer.send()</code> for attaching Kafka headers to messages</li> <li><code>MockDeadLetterQueue</code> in <code>typedkafka.testing</code> for unit testing DLQ logic without a broker</li> <li>New examples: <code>examples/metrics.py</code>, <code>examples/dead_letter_queue.py</code></li> <li>Updated examples: headers in <code>producer.py</code>, stats/security in <code>config_builders.py</code>, metrics and DLQ in <code>testing_mocks.py</code></li> <li>Test coverage: 320 tests (up from 294)</li> </ul>"},{"location":"changelog/#040","title":"[0.4.0]","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Consumer offset management: <code>seek()</code>, <code>assignment()</code>, <code>assign()</code>, <code>position()</code> methods on <code>KafkaConsumer</code></li> <li><code>poll_batch()</code> method on <code>KafkaConsumer</code> for consuming multiple messages at once</li> <li>Security config helpers: <code>sasl_plain()</code>, <code>sasl_scram()</code>, <code>ssl()</code> on both <code>ProducerConfig</code> and <code>ConsumerConfig</code></li> <li>Config validation: <code>build(validate=True)</code> checks for required fields (<code>bootstrap.servers</code>, <code>group.id</code>)</li> <li><code>MockMessage</code> now matches <code>KafkaMessage</code> interface: <code>value_as_string()</code>, <code>value_as_json()</code>, <code>key_as_string()</code>, <code>__repr__()</code></li> <li><code>MockConsumer</code> offset management: <code>seek()</code>, <code>assignment()</code>, <code>assign()</code>, <code>position()</code>, <code>poll_batch()</code></li> <li><code>DeliveryCallback</code> type alias for delivery report callbacks in producer and testing modules</li> <li><code>KafkaMessage</code> added to top-level <code>__all__</code> exports</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Async consumer <code>poll()</code> now returns <code>KafkaMessage</code> instead of raw confluent-kafka message</li> <li>Async consumer <code>__aiter__</code> yields <code>KafkaMessage</code> objects for API consistency</li> <li>Async producer and consumer docstrings now document ThreadPoolExecutor wrapping limitation</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Delivery callback type annotations now compatible with mypy strict checking</li> </ul>"},{"location":"changelog/#033","title":"[0.3.3]","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Separate <code>CHANGELOG.md</code> following Keep a Changelog format</li> <li><code>Makefile</code> with common dev tasks (<code>make test</code>, <code>make lint</code>, <code>make check</code>, etc.)</li> <li>Shared test fixtures in <code>tests/conftest.py</code></li> <li>Tests for <code>admin.py</code> and <code>aio.py</code> (test count: 120 \u2192 146)</li> <li>Python 3.13 to CI test matrix</li> <li>MkDocs documentation site with Material theme and auto-generated API reference</li> <li><code>docs</code> optional dependency group for documentation tooling</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Moved inline changelog from README to <code>CHANGELOG.md</code></li> <li>Updated documentation URL to GitHub Pages</li> </ul>"},{"location":"changelog/#032","title":"[0.3.2]","text":""},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Move code examples from README into standalone <code>examples/</code> directory</li> <li>Pin all CI/CD actions to commit SHAs for supply chain security</li> <li>Switch PyPI publishing to Trusted Publishers (OIDC) instead of API tokens</li> </ul>"},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li><code>py.typed</code> marker file for PEP 561 compliance</li> </ul>"},{"location":"changelog/#031","title":"[0.3.1]","text":""},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Update README, CONTRIBUTING, and SECURITY docs for v0.3.0 features</li> </ul>"},{"location":"changelog/#030","title":"[0.3.0]","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Transaction support: <code>init_transactions()</code>, <code>begin/commit/abort_transaction()</code>, <code>transaction()</code> context manager</li> <li>Async producer and consumer (<code>typedkafka.aio</code>)</li> <li>Retry utilities: <code>@retry</code> decorator and <code>RetryPolicy</code> class</li> <li>Pluggable serializers: <code>Serializer</code>/<code>Deserializer</code> ABCs, JSON, String, and Avro implementations</li> <li>Batch send: <code>send_batch()</code> on producer</li> <li>Consumer rebalance callbacks: <code>on_assign</code>, <code>on_revoke</code>, <code>on_lost</code> on <code>subscribe()</code></li> <li>Configurable iterator poll timeout via <code>poll_timeout</code> attribute</li> <li>Config validation: early <code>ValueError</code> on invalid <code>acks</code>, <code>compression</code>, <code>auto_offset_reset</code>, <code>linger_ms</code>, <code>batch_size</code></li> <li>Expanded test suite (120 tests)</li> </ul>"},{"location":"changelog/#020","title":"[0.2.0]","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Testing utilities (MockProducer, MockConsumer)</li> <li>Type-safe configuration builders (ProducerConfig, ConsumerConfig)</li> <li>Admin client wrapper for topic management</li> </ul>"},{"location":"changelog/#010","title":"[0.1.0]","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>Initial release with KafkaProducer, KafkaConsumer, full type hints, context manager support</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#producing-messages","title":"Producing Messages","text":"<pre><code>from typedkafka import KafkaProducer\n\nwith KafkaProducer({\"bootstrap.servers\": \"localhost:9092\"}) as producer:\n    producer.send(\"my-topic\", b\"Hello, Kafka!\")\n    producer.send_json(\"events\", {\"user_id\": 123, \"action\": \"click\"})\n    producer.send_string(\"logs\", \"Application started\")\n    producer.flush()\n</code></pre>"},{"location":"getting-started/#consuming-messages","title":"Consuming Messages","text":"<pre><code>from typedkafka import KafkaConsumer\n\nconfig = {\n    \"bootstrap.servers\": \"localhost:9092\",\n    \"group.id\": \"my-consumer-group\",\n    \"auto.offset.reset\": \"earliest\",\n}\n\nwith KafkaConsumer(config) as consumer:\n    consumer.subscribe([\"my-topic\"])\n    for msg in consumer:\n        data = msg.value_as_json()\n        print(f\"Received: {data}\")\n        consumer.commit(msg)\n</code></pre>"},{"location":"getting-started/#transactions","title":"Transactions","text":"<pre><code>from typedkafka import KafkaProducer\n\nproducer = KafkaProducer({\n    \"bootstrap.servers\": \"localhost:9092\",\n    \"transactional.id\": \"my-txn-id\",\n})\nproducer.init_transactions()\n\nwith producer.transaction():\n    producer.send(\"topic\", b\"msg1\")\n    producer.send(\"topic\", b\"msg2\")\n    # Commits on success, aborts on exception\n</code></pre>"},{"location":"getting-started/#async","title":"Async","text":"<pre><code>from typedkafka.aio import AsyncKafkaProducer, AsyncKafkaConsumer\n\nasync with AsyncKafkaProducer({\"bootstrap.servers\": \"localhost:9092\"}) as producer:\n    await producer.send(\"topic\", b\"async message\")\n    await producer.send_json(\"events\", {\"id\": 1})\n    await producer.flush()\n</code></pre>"},{"location":"getting-started/#retry","title":"Retry","text":"<pre><code>from typedkafka.retry import retry, RetryPolicy\n\n@retry(max_attempts=3, backoff_base=1.0)\ndef send_with_retry(producer, data):\n    producer.send_json(\"events\", data)\n    producer.flush()\n</code></pre>"},{"location":"getting-started/#serializers","title":"Serializers","text":"<pre><code>from typedkafka.serializers import JsonSerializer\n\njson_ser = JsonSerializer()\ndata = json_ser.serialize(\"topic\", {\"user_id\": 123})\n</code></pre>"},{"location":"getting-started/#type-safe-configuration","title":"Type-Safe Configuration","text":"<pre><code>from typedkafka import ProducerConfig, KafkaProducer\n\nconfig = (ProducerConfig()\n    .bootstrap_servers(\"localhost:9092\")\n    .acks(\"all\")\n    .compression(\"gzip\")\n    .linger_ms(10)\n    .build())\n\nproducer = KafkaProducer(config)\n</code></pre>"},{"location":"getting-started/#message-headers","title":"Message Headers","text":"<pre><code>from typedkafka import KafkaProducer\n\nwith KafkaProducer({\"bootstrap.servers\": \"localhost:9092\"}) as producer:\n    producer.send(\"events\", b\"traced\", headers=[(\"trace-id\", b\"abc123\")])\n    producer.flush()\n</code></pre>"},{"location":"getting-started/#metrics","title":"Metrics","text":"<pre><code>from typedkafka import KafkaProducer\n\nproducer = KafkaProducer(\n    {\"bootstrap.servers\": \"localhost:9092\", \"statistics.interval.ms\": 5000}\n)\nproducer.send(\"events\", b\"hello\")\nprint(f\"Messages sent: {producer.metrics.messages_sent}\")\nprint(f\"Bytes sent: {producer.metrics.bytes_sent}\")\nproducer.flush()\n</code></pre>"},{"location":"getting-started/#dead-letter-queue","title":"Dead Letter Queue","text":"<pre><code>from typedkafka import DeadLetterQueue, KafkaConsumer, KafkaProducer, process_with_dlq\n\nproducer = KafkaProducer({\"bootstrap.servers\": \"localhost:9092\"})\ndlq = DeadLetterQueue(producer)  # failed messages go to \"&lt;topic&gt;.dlq\"\n\nconsumer = KafkaConsumer(\n    {\"bootstrap.servers\": \"localhost:9092\", \"group.id\": \"my-group\"}\n)\nconsumer.subscribe([\"orders\"])\n\nfor msg in consumer:\n    success = process_with_dlq(msg, lambda m: process_order(m.value_as_json()), dlq)\n    if success:\n        consumer.commit(msg)\n</code></pre>"},{"location":"getting-started/#security-configuration","title":"Security Configuration","text":"<pre><code>from typedkafka import ProducerConfig, KafkaProducer\n\nconfig = (\n    ProducerConfig()\n    .bootstrap_servers(\"kafka.example.com:9093\")\n    .sasl_scram(\"user\", \"password\")\n    .acks(\"all\")\n    .build(validate=True)\n)\n\nproducer = KafkaProducer(config)\n</code></pre>"},{"location":"getting-started/#testing","title":"Testing","text":"<pre><code>from typedkafka.testing import MockProducer, MockConsumer\n\ndef test_my_producer():\n    producer = MockProducer()\n    producer.send_json(\"events\", {\"user_id\": 123})\n    producer.flush()\n    assert len(producer.messages[\"events\"]) == 1\n    assert producer.metrics.messages_sent == 1\n</code></pre>"},{"location":"api/admin/","title":"Admin","text":""},{"location":"api/admin/#typedkafka.admin.KafkaAdmin","title":"<code>typedkafka.admin.KafkaAdmin</code>","text":"<p>A well-documented Kafka admin client with full type hints.</p> <p>Provides methods for managing topics, configurations, and cluster operations with comprehensive documentation and better error messages.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; admin = KafkaAdmin({\"bootstrap.servers\": \"localhost:9092\"})\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a topic\n&gt;&gt;&gt; admin.create_topic(\"events\", num_partitions=3, replication_factor=2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List all topics\n&gt;&gt;&gt; topics = admin.list_topics()\n&gt;&gt;&gt; print(topics)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Delete a topic\n&gt;&gt;&gt; admin.delete_topic(\"old-topic\")\n</code></pre> <p>Attributes:</p> Name Type Description <code>config</code> <p>The configuration dictionary used to initialize the admin client</p>"},{"location":"api/admin/#typedkafka.admin.KafkaAdmin.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize a Kafka admin client.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary. Required option: - bootstrap.servers (str): Comma-separated broker addresses</p> required <p>Raises:</p> Type Description <code>AdminError</code> <p>If the admin client cannot be initialized</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; admin = KafkaAdmin({\"bootstrap.servers\": \"localhost:9092\"})\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With multiple brokers\n&gt;&gt;&gt; admin = KafkaAdmin({\n...     \"bootstrap.servers\": \"broker1:9092,broker2:9092,broker3:9092\"\n... })\n</code></pre>"},{"location":"api/admin/#typedkafka.admin.KafkaAdmin.create_topic","title":"<code>create_topic(topic, num_partitions=1, replication_factor=1, config=None, timeout=30.0)</code>","text":"<p>Create a new Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Topic name to create</p> required <code>num_partitions</code> <code>int</code> <p>Number of partitions (default: 1)</p> <code>1</code> <code>replication_factor</code> <code>int</code> <p>Replication factor (default: 1, recommended: 2-3)</p> <code>1</code> <code>config</code> <code>Optional[dict[str, str]]</code> <p>Optional topic configuration dict</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Operation timeout in seconds (default: 30.0)</p> <code>30.0</code> <p>Raises:</p> Type Description <code>AdminError</code> <p>If topic creation fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; admin = KafkaAdmin({\"bootstrap.servers\": \"localhost:9092\"})\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple topic creation\n&gt;&gt;&gt; admin.create_topic(\"my-topic\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Topic with multiple partitions and replication\n&gt;&gt;&gt; admin.create_topic(\"events\", num_partitions=10, replication_factor=3)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Topic with custom configuration\n&gt;&gt;&gt; admin.create_topic(\n...     \"logs\",\n...     num_partitions=5,\n...     config={\"retention.ms\": \"604800000\"}  # 7 days\n... )\n</code></pre>"},{"location":"api/admin/#typedkafka.admin.KafkaAdmin.delete_topic","title":"<code>delete_topic(topic, timeout=30.0)</code>","text":"<p>Delete a Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Topic name to delete</p> required <code>timeout</code> <code>float</code> <p>Operation timeout in seconds (default: 30.0)</p> <code>30.0</code> <p>Raises:</p> Type Description <code>AdminError</code> <p>If topic deletion fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; admin = KafkaAdmin({\"bootstrap.servers\": \"localhost:9092\"})\n&gt;&gt;&gt; admin.delete_topic(\"old-topic\")\n</code></pre>"},{"location":"api/admin/#typedkafka.admin.KafkaAdmin.describe_topic","title":"<code>describe_topic(topic, timeout=10.0)</code>","text":"<p>Get detailed information about a topic.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Topic name</p> required <code>timeout</code> <code>float</code> <p>Request timeout in seconds (default: 10.0)</p> <code>10.0</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict containing topic metadata (partitions, replication, etc.)</p> <p>Raises:</p> Type Description <code>AdminError</code> <p>If describing the topic fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; admin = KafkaAdmin({\"bootstrap.servers\": \"localhost:9092\"})\n&gt;&gt;&gt; info = admin.describe_topic(\"my-topic\")\n&gt;&gt;&gt; print(f\"Partitions: {len(info['partitions'])}\")\n</code></pre>"},{"location":"api/admin/#typedkafka.admin.KafkaAdmin.list_topics","title":"<code>list_topics(timeout=10.0)</code>","text":"<p>List all topics in the Kafka cluster.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Request timeout in seconds (default: 10.0)</p> <code>10.0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of topic names</p> <p>Raises:</p> Type Description <code>AdminError</code> <p>If listing topics fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; admin = KafkaAdmin({\"bootstrap.servers\": \"localhost:9092\"})\n&gt;&gt;&gt; topics = admin.list_topics()\n&gt;&gt;&gt; for topic in topics:\n...     print(f\"Topic: {topic}\")\n</code></pre>"},{"location":"api/admin/#typedkafka.admin.KafkaAdmin.topic_exists","title":"<code>topic_exists(topic, timeout=10.0)</code>","text":"<p>Check if a topic exists.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Topic name to check</p> required <code>timeout</code> <code>float</code> <p>Request timeout in seconds (default: 10.0)</p> <code>10.0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if topic exists, False otherwise</p> <p>Raises:</p> Type Description <code>AdminError</code> <p>If the check fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; admin = KafkaAdmin({\"bootstrap.servers\": \"localhost:9092\"})\n&gt;&gt;&gt; if admin.topic_exists(\"my-topic\"):\n...     print(\"Topic exists!\")\n... else:\n...     admin.create_topic(\"my-topic\")\n</code></pre>"},{"location":"api/admin/#typedkafka.admin.TopicConfig","title":"<code>typedkafka.admin.TopicConfig</code>","text":"<p>Configuration for creating a new Kafka topic.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = (TopicConfig(\"my-topic\")\n...     .partitions(3)\n...     .replication_factor(2)\n...     .config(\"retention.ms\", \"86400000\"))  # 1 day retention\n</code></pre>"},{"location":"api/admin/#typedkafka.admin.TopicConfig.__init__","title":"<code>__init__(name)</code>","text":"<p>Initialize topic configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Topic name</p> required"},{"location":"api/admin/#typedkafka.admin.TopicConfig.config","title":"<code>config(key, value)</code>","text":"<p>Set a topic configuration parameter.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Configuration key (e.g., \"retention.ms\", \"compression.type\")</p> required <code>value</code> <code>str</code> <p>Configuration value</p> required <p>Returns:</p> Type Description <code>TopicConfig</code> <p>Self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = (TopicConfig(\"logs\")\n...     .config(\"retention.ms\", \"604800000\")  # 7 days\n...     .config(\"compression.type\", \"gzip\"))\n</code></pre>"},{"location":"api/admin/#typedkafka.admin.TopicConfig.partitions","title":"<code>partitions(count)</code>","text":"<p>Set number of partitions.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of partitions (must be &gt;= 1)</p> required <p>Returns:</p> Type Description <code>TopicConfig</code> <p>Self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = TopicConfig(\"my-topic\").partitions(10)\n</code></pre>"},{"location":"api/admin/#typedkafka.admin.TopicConfig.replication_factor","title":"<code>replication_factor(factor)</code>","text":"<p>Set replication factor.</p> <p>Parameters:</p> Name Type Description Default <code>factor</code> <code>int</code> <p>Replication factor (typically 2 or 3)</p> required <p>Returns:</p> Type Description <code>TopicConfig</code> <p>Self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = TopicConfig(\"my-topic\").replication_factor(3)\n</code></pre>"},{"location":"api/admin/#typedkafka.admin.AdminError","title":"<code>typedkafka.admin.AdminError</code>","text":"<p>               Bases: <code>KafkaError</code></p> <p>Raised when an admin operation fails.</p>"},{"location":"api/async/","title":"Async","text":""},{"location":"api/async/#typedkafka.aio.AsyncKafkaProducer","title":"<code>typedkafka.aio.AsyncKafkaProducer</code>","text":"<p>Async Kafka producer wrapping confluent-kafka with asyncio support.</p> <p>Uses a thread pool to run confluent-kafka's synchronous operations without blocking the event loop.</p> Note <p>This implementation uses a ThreadPoolExecutor to wrap synchronous confluent-kafka calls. It does not provide true non-blocking async I/O. Each blocking call is offloaded to a thread pool. For high-throughput scenarios, consider tuning the executor's max_workers parameter.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; async with AsyncKafkaProducer({\"bootstrap.servers\": \"localhost:9092\"}) as producer:\n...     await producer.send(\"topic\", b\"message\")\n...     await producer.flush()\n</code></pre> <p>Attributes:</p> Name Type Description <code>config</code> <p>The configuration dictionary used to initialize the producer</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaProducer.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Async context manager entry.</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaProducer.__aexit__","title":"<code>__aexit__(exc_type, exc_val, exc_tb)</code>  <code>async</code>","text":"<p>Async context manager exit.</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaProducer.__init__","title":"<code>__init__(config, executor=None)</code>","text":"<p>Initialize an async Kafka producer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary for the producer.</p> required <code>executor</code> <code>Optional[ThreadPoolExecutor]</code> <p>Optional ThreadPoolExecutor. If None, a default one is created.</p> <code>None</code> <p>Raises:</p> Type Description <code>ProducerError</code> <p>If the producer cannot be initialized.</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaProducer.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Flush and close the producer.</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaProducer.flush","title":"<code>flush(timeout=-1)</code>  <code>async</code>","text":"<p>Asynchronously wait for all queued messages to be delivered.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum time to wait in seconds. -1 for infinite.</p> <code>-1</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of messages still in queue.</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaProducer.send","title":"<code>send(topic, value, key=None, partition=None)</code>  <code>async</code>","text":"<p>Asynchronously send a message to a Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>The topic name to send the message to.</p> required <code>value</code> <code>bytes</code> <p>The message payload as bytes.</p> required <code>key</code> <code>Optional[bytes]</code> <p>Optional message key as bytes.</p> <code>None</code> <code>partition</code> <code>Optional[int]</code> <p>Optional partition number.</p> <code>None</code> <p>Raises:</p> Type Description <code>ProducerError</code> <p>If the message cannot be queued.</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaProducer.send_json","title":"<code>send_json(topic, value, key=None, partition=None)</code>  <code>async</code>","text":"<p>Asynchronously send a JSON-serialized message.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>The topic name.</p> required <code>value</code> <code>Any</code> <p>Any JSON-serializable Python object.</p> required <code>key</code> <code>Optional[str]</code> <p>Optional string key.</p> <code>None</code> <code>partition</code> <code>Optional[int]</code> <p>Optional partition number.</p> <code>None</code> <p>Raises:</p> Type Description <code>SerializationError</code> <p>If JSON serialization fails.</p> <code>ProducerError</code> <p>If the message cannot be queued.</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaProducer.send_string","title":"<code>send_string(topic, value, key=None, partition=None)</code>  <code>async</code>","text":"<p>Asynchronously send a UTF-8 encoded string message.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>The topic name.</p> required <code>value</code> <code>str</code> <p>String message to send.</p> required <code>key</code> <code>Optional[str]</code> <p>Optional string key.</p> <code>None</code> <code>partition</code> <code>Optional[int]</code> <p>Optional partition number.</p> <code>None</code> <p>Raises:</p> Type Description <code>ProducerError</code> <p>If the message cannot be queued.</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaConsumer","title":"<code>typedkafka.aio.AsyncKafkaConsumer</code>","text":"<p>Async Kafka consumer wrapping confluent-kafka with asyncio support.</p> <p>Uses a thread pool to run confluent-kafka's synchronous poll without blocking the event loop. Supports <code>async for</code> iteration.</p> Note <p>This implementation uses a ThreadPoolExecutor to wrap synchronous confluent-kafka calls. It does not provide true non-blocking async I/O. Each blocking call is offloaded to a thread pool. For high-throughput scenarios, consider tuning the executor's max_workers parameter.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; async with AsyncKafkaConsumer(config) as consumer:\n...     consumer.subscribe([\"topic\"])\n...     async for msg in consumer:\n...         print(msg.value_as_string())\n</code></pre> <p>Attributes:</p> Name Type Description <code>config</code> <p>The configuration dictionary used to initialize the consumer</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaConsumer.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Async context manager entry.</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaConsumer.__aexit__","title":"<code>__aexit__(exc_type, exc_val, exc_tb)</code>  <code>async</code>","text":"<p>Async context manager exit.</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaConsumer.__aiter__","title":"<code>__aiter__()</code>  <code>async</code>","text":"<p>Async iterate over messages indefinitely.</p> <p>Yields:</p> Type Description <code>AsyncIterator[KafkaMessage]</code> <p>KafkaMessage objects as they arrive.</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaConsumer.__init__","title":"<code>__init__(config, executor=None)</code>","text":"<p>Initialize an async Kafka consumer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary for the consumer.</p> required <code>executor</code> <code>Optional[ThreadPoolExecutor]</code> <p>Optional ThreadPoolExecutor.</p> <code>None</code> <p>Raises:</p> Type Description <code>ConsumerError</code> <p>If the consumer cannot be initialized.</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaConsumer.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the consumer and leave the consumer group.</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaConsumer.commit","title":"<code>commit(message=None, asynchronous=True)</code>  <code>async</code>","text":"<p>Asynchronously commit offsets.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Any</code> <p>Specific message to commit. If None, commits all consumed.</p> <code>None</code> <code>asynchronous</code> <code>bool</code> <p>If True, commit asynchronously.</p> <code>True</code>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaConsumer.poll","title":"<code>poll(timeout=1.0)</code>  <code>async</code>","text":"<p>Asynchronously poll for a single message.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum time to wait in seconds.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Optional[KafkaMessage]</code> <p>A KafkaMessage, or None if timeout expired.</p> <p>Raises:</p> Type Description <code>ConsumerError</code> <p>If an error occurs during polling.</p>"},{"location":"api/async/#typedkafka.aio.AsyncKafkaConsumer.subscribe","title":"<code>subscribe(topics, **kwargs)</code>","text":"<p>Subscribe to topics.</p> <p>Parameters:</p> Name Type Description Default <code>topics</code> <code>list[str]</code> <p>List of topic names.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to confluent-kafka subscribe.</p> <code>{}</code>"},{"location":"api/config/","title":"Configuration","text":""},{"location":"api/config/#typedkafka.config.ProducerConfig","title":"<code>typedkafka.config.ProducerConfig</code>","text":"<p>               Bases: <code>_SecurityConfigMixin</code></p> <p>Type-safe builder for Kafka producer configuration.</p> <p>Provides a fluent API with full type hints and validation for common producer configuration options.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = (ProducerConfig()\n...     .bootstrap_servers(\"localhost:9092\")\n...     .compression(\"gzip\")\n...     .acks(\"all\")\n...     .build())\n&gt;&gt;&gt;\n&gt;&gt;&gt; from typedkafka import KafkaProducer\n&gt;&gt;&gt; producer = KafkaProducer(config)\n</code></pre> <pre><code>&gt;&gt;&gt; # With multiple brokers\n&gt;&gt;&gt; config = (ProducerConfig()\n...     .bootstrap_servers(\"broker1:9092,broker2:9092,broker3:9092\")\n...     .client_id(\"my-application\")\n...     .build())\n</code></pre>"},{"location":"api/config/#typedkafka.config.ProducerConfig.__init__","title":"<code>__init__()</code>","text":"<p>Initialize an empty producer configuration.</p>"},{"location":"api/config/#typedkafka.config.ProducerConfig.acks","title":"<code>acks(acks)</code>","text":"<p>Set the number of acknowledgments required.</p> <p>Parameters:</p> Name Type Description Default <code>acks</code> <code>Union[Literal['0', '1', 'all'], int]</code> <p>Acknowledgment level: - \"0\" or 0: No acknowledgment (fire and forget) - \"1\" or 1: Leader acknowledgment only - \"all\" or -1: All in-sync replicas must acknowledge</p> required <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = ProducerConfig().acks(\"all\")  # Maximum durability\n&gt;&gt;&gt; config = ProducerConfig().acks(\"1\")     # Leader only\n&gt;&gt;&gt; config = ProducerConfig().acks(\"0\")     # No acknowledgment\n</code></pre>"},{"location":"api/config/#typedkafka.config.ProducerConfig.batch_size","title":"<code>batch_size(bytes_size)</code>","text":"<p>Set maximum batch size in bytes.</p> <p>Parameters:</p> Name Type Description Default <code>bytes_size</code> <code>int</code> <p>Maximum batch size (default: 16384)</p> required <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = ProducerConfig().batch_size(32768)  # 32KB batches\n</code></pre>"},{"location":"api/config/#typedkafka.config.ProducerConfig.bootstrap_servers","title":"<code>bootstrap_servers(servers)</code>","text":"<p>Set the Kafka broker addresses.</p> <p>Parameters:</p> Name Type Description Default <code>servers</code> <code>str</code> <p>Comma-separated list of broker addresses. Example: \"localhost:9092\" or \"broker1:9092,broker2:9092\"</p> required <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = ProducerConfig().bootstrap_servers(\"localhost:9092\")\n&gt;&gt;&gt; config = ProducerConfig().bootstrap_servers(\"b1:9092,b2:9092,b3:9092\")\n</code></pre>"},{"location":"api/config/#typedkafka.config.ProducerConfig.build","title":"<code>build(validate=False)</code>","text":"<p>Build and return the configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>validate</code> <code>bool</code> <p>If True, verify required fields and configuration consistency.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Configuration dict ready for KafkaProducer</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If validate is True and configuration is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = (ProducerConfig()\n...     .bootstrap_servers(\"localhost:9092\")\n...     .acks(\"all\")\n...     .build())\n&gt;&gt;&gt; from typedkafka import KafkaProducer\n&gt;&gt;&gt; producer = KafkaProducer(config)\n</code></pre>"},{"location":"api/config/#typedkafka.config.ProducerConfig.client_id","title":"<code>client_id(client_id)</code>","text":"<p>Set the client ID for this producer.</p> <p>Parameters:</p> Name Type Description Default <code>client_id</code> <code>str</code> <p>Client identifier string</p> required <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#typedkafka.config.ProducerConfig.compression","title":"<code>compression(compression_type)</code>","text":"<p>Set the compression codec.</p> <p>Parameters:</p> Name Type Description Default <code>compression_type</code> <code>Literal['none', 'gzip', 'snappy', 'lz4', 'zstd']</code> <p>Compression algorithm to use</p> required <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = ProducerConfig().compression(\"gzip\")\n&gt;&gt;&gt; config = ProducerConfig().compression(\"zstd\")  # Best compression\n</code></pre>"},{"location":"api/config/#typedkafka.config.ProducerConfig.enable_idempotence","title":"<code>enable_idempotence(enabled=True)</code>","text":"<p>Enable idempotent producer (requires acks=all).</p> <p>When enabled, the producer will ensure that messages are delivered exactly once and in order per partition.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>True to enable idempotence (default: True)</p> <code>True</code> <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#typedkafka.config.ProducerConfig.exactly_once","title":"<code>exactly_once(bootstrap_servers, transactional_id)</code>  <code>classmethod</code>","text":"<p>Preset for exactly-once semantics.</p> <p>Configures idempotent, transactional producer with acks=all.</p> <p>Parameters:</p> Name Type Description Default <code>bootstrap_servers</code> <code>str</code> <p>Broker addresses</p> required <code>transactional_id</code> <code>str</code> <p>Unique transactional identifier</p> required <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Pre-configured ProducerConfig</p>"},{"location":"api/config/#typedkafka.config.ProducerConfig.from_env","title":"<code>from_env(prefix='KAFKA_')</code>  <code>classmethod</code>","text":"<p>Load configuration from environment variables.</p> <p>Reads environment variables with the given prefix and maps them to producer configuration options.</p> Supported variables <ul> <li><code>{prefix}BOOTSTRAP_SERVERS</code></li> <li><code>{prefix}ACKS</code></li> <li><code>{prefix}COMPRESSION</code></li> <li><code>{prefix}LINGER_MS</code></li> <li><code>{prefix}TRANSACTIONAL_ID</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Environment variable prefix (default: \"KAFKA_\")</p> <code>'KAFKA_'</code> <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>ProducerConfig with values from environment</p>"},{"location":"api/config/#typedkafka.config.ProducerConfig.high_throughput","title":"<code>high_throughput(bootstrap_servers)</code>  <code>classmethod</code>","text":"<p>Preset for high-throughput scenarios.</p> <p>Configures lz4 compression, acks=1, batching with 50ms linger and 64KB batch size.</p> <p>Parameters:</p> Name Type Description Default <code>bootstrap_servers</code> <code>str</code> <p>Broker addresses</p> required <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Pre-configured ProducerConfig</p>"},{"location":"api/config/#typedkafka.config.ProducerConfig.linger_ms","title":"<code>linger_ms(milliseconds)</code>","text":"<p>Set time to wait before sending a batch.</p> <p>Parameters:</p> Name Type Description Default <code>milliseconds</code> <code>int</code> <p>Time to wait for more messages before sending</p> required <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Wait up to 10ms to batch messages\n&gt;&gt;&gt; config = ProducerConfig().linger_ms(10)\n</code></pre>"},{"location":"api/config/#typedkafka.config.ProducerConfig.max_in_flight_requests","title":"<code>max_in_flight_requests(count)</code>","text":"<p>Set maximum number of unacknowledged requests.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Max in-flight requests per connection (1-5 recommended)</p> required <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#typedkafka.config.ProducerConfig.retries","title":"<code>retries(count)</code>","text":"<p>Set number of retries for failed sends.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of retries (default: 2147483647 for infinite)</p> required <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#typedkafka.config.ProducerConfig.set","title":"<code>set(key, value)</code>","text":"<p>Set a custom configuration parameter.</p> <p>Use this for advanced configurations not covered by type-safe methods.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Configuration key</p> required <code>value</code> <code>Any</code> <p>Configuration value</p> required <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = ProducerConfig().set(\"queue.buffering.max.messages\", 100000)\n</code></pre>"},{"location":"api/config/#typedkafka.config.ProducerConfig.stats_interval_ms","title":"<code>stats_interval_ms(milliseconds)</code>","text":"<p>Enable statistics reporting at the given interval.</p> <p>When set, confluent-kafka will emit internal statistics at this interval. Use the <code>on_stats</code> parameter on <code>KafkaProducer</code> to receive parsed stats.</p> <p>Parameters:</p> Name Type Description Default <code>milliseconds</code> <code>int</code> <p>Stats reporting interval in milliseconds (e.g. 5000 for every 5s).</p> required <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Self for method chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If milliseconds is negative.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = ProducerConfig().stats_interval_ms(5000).build()\n</code></pre>"},{"location":"api/config/#typedkafka.config.ProducerConfig.transactional_id","title":"<code>transactional_id(txn_id)</code>","text":"<p>Set the transactional ID for exactly-once semantics.</p> <p>Requires <code>enable_idempotence(True)</code> and <code>acks(\"all\")</code>.</p> <p>Parameters:</p> Name Type Description Default <code>txn_id</code> <code>str</code> <p>Unique transactional identifier</p> required <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#typedkafka.config.ConsumerConfig","title":"<code>typedkafka.config.ConsumerConfig</code>","text":"<p>               Bases: <code>_SecurityConfigMixin</code></p> <p>Type-safe builder for Kafka consumer configuration.</p> <p>Provides a fluent API with full type hints and validation for common consumer configuration options.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = (ConsumerConfig()\n...     .bootstrap_servers(\"localhost:9092\")\n...     .group_id(\"my-consumer-group\")\n...     .auto_offset_reset(\"earliest\")\n...     .build())\n&gt;&gt;&gt;\n&gt;&gt;&gt; from typedkafka import KafkaConsumer\n&gt;&gt;&gt; consumer = KafkaConsumer(config)\n</code></pre>"},{"location":"api/config/#typedkafka.config.ConsumerConfig.__init__","title":"<code>__init__()</code>","text":"<p>Initialize an empty consumer configuration.</p>"},{"location":"api/config/#typedkafka.config.ConsumerConfig.auto_commit_interval_ms","title":"<code>auto_commit_interval_ms(milliseconds)</code>","text":"<p>Set frequency of automatic offset commits.</p> <p>Parameters:</p> Name Type Description Default <code>milliseconds</code> <code>int</code> <p>Commit interval (default: 5000)</p> required <p>Returns:</p> Type Description <code>ConsumerConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#typedkafka.config.ConsumerConfig.auto_offset_reset","title":"<code>auto_offset_reset(reset)</code>","text":"<p>Set behavior when no initial offset exists.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>Literal['earliest', 'latest', 'none']</code> <p>Offset reset behavior: - \"earliest\": Start from the beginning - \"latest\": Start from the end (skip existing messages) - \"none\": Throw error if no offset exists</p> required <p>Returns:</p> Type Description <code>ConsumerConfig</code> <p>Self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Process all messages from the beginning\n&gt;&gt;&gt; config = ConsumerConfig().auto_offset_reset(\"earliest\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Only process new messages\n&gt;&gt;&gt; config = ConsumerConfig().auto_offset_reset(\"latest\")\n</code></pre>"},{"location":"api/config/#typedkafka.config.ConsumerConfig.bootstrap_servers","title":"<code>bootstrap_servers(servers)</code>","text":"<p>Set the Kafka broker addresses.</p> <p>Parameters:</p> Name Type Description Default <code>servers</code> <code>str</code> <p>Comma-separated list of broker addresses</p> required <p>Returns:</p> Type Description <code>ConsumerConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#typedkafka.config.ConsumerConfig.build","title":"<code>build(validate=False)</code>","text":"<p>Build and return the configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>validate</code> <code>bool</code> <p>If True, verify that required fields (bootstrap.servers, group.id) are set.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Configuration dict ready for KafkaConsumer</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If validate is True and required fields are missing.</p>"},{"location":"api/config/#typedkafka.config.ConsumerConfig.client_id","title":"<code>client_id(client_id)</code>","text":"<p>Set the client ID for this consumer.</p> <p>Parameters:</p> Name Type Description Default <code>client_id</code> <code>str</code> <p>Client identifier string</p> required <p>Returns:</p> Type Description <code>ConsumerConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#typedkafka.config.ConsumerConfig.enable_auto_commit","title":"<code>enable_auto_commit(enabled=True)</code>","text":"<p>Enable or disable automatic offset commits.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>True to auto-commit, False for manual commits</p> <code>True</code> <p>Returns:</p> Type Description <code>ConsumerConfig</code> <p>Self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Manual offset management\n&gt;&gt;&gt; config = ConsumerConfig().enable_auto_commit(False)\n</code></pre>"},{"location":"api/config/#typedkafka.config.ConsumerConfig.from_env","title":"<code>from_env(prefix='KAFKA_')</code>  <code>classmethod</code>","text":"<p>Load configuration from environment variables.</p> Supported variables <ul> <li><code>{prefix}BOOTSTRAP_SERVERS</code></li> <li><code>{prefix}GROUP_ID</code></li> <li><code>{prefix}AUTO_OFFSET_RESET</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Environment variable prefix (default: \"KAFKA_\")</p> <code>'KAFKA_'</code> <p>Returns:</p> Type Description <code>ConsumerConfig</code> <p>ConsumerConfig with values from environment</p>"},{"location":"api/config/#typedkafka.config.ConsumerConfig.group_id","title":"<code>group_id(group_id)</code>","text":"<p>Set the consumer group ID (required for subscribe()).</p> <p>Parameters:</p> Name Type Description Default <code>group_id</code> <code>str</code> <p>Consumer group identifier</p> required <p>Returns:</p> Type Description <code>ConsumerConfig</code> <p>Self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = ConsumerConfig().group_id(\"my-application-consumers\")\n</code></pre>"},{"location":"api/config/#typedkafka.config.ConsumerConfig.max_poll_interval_ms","title":"<code>max_poll_interval_ms(milliseconds)</code>","text":"<p>Set maximum time between polls.</p> <p>Parameters:</p> Name Type Description Default <code>milliseconds</code> <code>int</code> <p>Max poll interval (default: 300000)</p> required <p>Returns:</p> Type Description <code>ConsumerConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#typedkafka.config.ConsumerConfig.max_poll_records","title":"<code>max_poll_records(count)</code>","text":"<p>Set maximum records returned in a single poll.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Max records per poll</p> required <p>Returns:</p> Type Description <code>ConsumerConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#typedkafka.config.ConsumerConfig.session_timeout_ms","title":"<code>session_timeout_ms(milliseconds)</code>","text":"<p>Set consumer session timeout.</p> <p>Parameters:</p> Name Type Description Default <code>milliseconds</code> <code>int</code> <p>Session timeout (default: 10000)</p> required <p>Returns:</p> Type Description <code>ConsumerConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#typedkafka.config.ConsumerConfig.set","title":"<code>set(key, value)</code>","text":"<p>Set a custom configuration parameter.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Configuration key</p> required <code>value</code> <code>Any</code> <p>Configuration value</p> required <p>Returns:</p> Type Description <code>ConsumerConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#typedkafka.config.ConsumerConfig.stats_interval_ms","title":"<code>stats_interval_ms(milliseconds)</code>","text":"<p>Enable statistics reporting at the given interval.</p> <p>When set, confluent-kafka will emit internal statistics at this interval. Use the <code>on_stats</code> parameter on <code>KafkaConsumer</code> to receive parsed stats.</p> <p>Parameters:</p> Name Type Description Default <code>milliseconds</code> <code>int</code> <p>Stats reporting interval in milliseconds (e.g. 5000 for every 5s).</p> required <p>Returns:</p> Type Description <code>ConsumerConfig</code> <p>Self for method chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If milliseconds is negative.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = ConsumerConfig().stats_interval_ms(5000).build()\n</code></pre>"},{"location":"api/consumer/","title":"Consumer","text":""},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer","title":"<code>typedkafka.consumer.KafkaConsumer</code>","text":"<p>A well-documented Kafka consumer with full type hints.</p> <p>This class wraps confluent-kafka's Consumer with: - Comprehensive docstrings on every method - Full type hints for IDE autocomplete - Better error messages - Convenient message deserialization methods - Context manager support for automatic cleanup - Iterator protocol for easy message consumption</p> Basic Usage <p>consumer = KafkaConsumer({ ...     \"bootstrap.servers\": \"localhost:9092\", ...     \"group.id\": \"my-group\", ...     \"auto.offset.reset\": \"earliest\" ... }) consumer.subscribe([\"my-topic\"]) for msg in consumer: ...     print(f\"Received: {msg.value_as_string()}\")</p> With Context Manager <p>with KafkaConsumer(config) as consumer: ...     consumer.subscribe([\"topic\"]) ...     for msg in consumer: ...         process(msg)</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>The configuration dictionary used to initialize the consumer</p>"},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer.metrics","title":"<code>metrics</code>  <code>property</code>","text":"<p>Current metrics for this consumer.</p> <p>Tracks messages received, errors, and (if stats enabled) byte throughput.</p>"},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter context manager.</p>"},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Exit context manager and cleanup resources.</p>"},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer.__init__","title":"<code>__init__(config, on_stats=None, value_deserializer=None)</code>","text":"<p>Initialize a Kafka consumer with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary for the consumer. Common options: - bootstrap.servers (str): Comma-separated list of broker addresses - group.id (str): Consumer group ID (required for subscribe()) - client.id (str): An identifier for this client - auto.offset.reset (str): What to do when there's no initial offset   \"earliest\" = start from beginning, \"latest\" = start from end - enable.auto.commit (bool): Automatically commit offsets (default: True) - auto.commit.interval.ms (int): Frequency of offset commits in milliseconds - max.poll.interval.ms (int): Max time between polls before being kicked from group - session.timeout.ms (int): Timeout for detecting consumer failures - statistics.interval.ms (int): Stats reporting interval in milliseconds</p> required <code>on_stats</code> <code>Optional[StatsCallback]</code> <p>Optional callback receiving parsed KafkaStats each reporting interval. Requires <code>statistics.interval.ms</code> to be set in config.</p> <code>None</code> <code>value_deserializer</code> <code>Optional[Callable[[bytes], Any]]</code> <p>Optional function to automatically deserialize message values. When set, use <code>msg.value_as(deserializer)</code> or the configured deserializer will be available for typed consumption patterns.</p> <code>None</code> <p>Raises:</p> Type Description <code>ConsumerError</code> <p>If the consumer cannot be initialized</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic consumer\n&gt;&gt;&gt; consumer = KafkaConsumer({\n...     \"bootstrap.servers\": \"localhost:9092\",\n...     \"group.id\": \"my-consumer-group\",\n...     \"auto.offset.reset\": \"earliest\"\n... })\n</code></pre> <pre><code>&gt;&gt;&gt; # Consumer with metrics\n&gt;&gt;&gt; consumer = KafkaConsumer({\n...     \"bootstrap.servers\": \"localhost:9092\",\n...     \"group.id\": \"my-group\",\n...     \"statistics.interval.ms\": 5000,\n... })\n&gt;&gt;&gt; print(consumer.metrics.messages_received)\n</code></pre>"},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over messages indefinitely.</p> <p>Uses the configured poll_timeout (default 1.0s). Configure via the poll_timeout property.</p> <p>Yields:</p> Type Description <code>KafkaMessage</code> <p>KafkaMessage objects as they arrive</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; for msg in consumer:\n...     print(f\"Received: {msg.value_as_string()}\")\n...     consumer.commit(msg)\n</code></pre> <pre><code>&gt;&gt;&gt; # With custom poll timeout\n&gt;&gt;&gt; consumer.poll_timeout = 5.0\n&gt;&gt;&gt; for msg in consumer:\n...     process(msg)\n</code></pre>"},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer.assign","title":"<code>assign(partitions)</code>","text":"<p>Manually assign partitions to this consumer.</p> <p>Parameters:</p> Name Type Description Default <code>partitions</code> <code>list[Any]</code> <p>List of TopicPartition objects to assign.</p> required <p>Raises:</p> Type Description <code>ConsumerError</code> <p>If the assignment fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from confluent_kafka import TopicPartition\n&gt;&gt;&gt; consumer.assign([TopicPartition(\"my-topic\", 0), TopicPartition(\"my-topic\", 1)])\n</code></pre>"},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer.assignment","title":"<code>assignment()</code>","text":"<p>Get the current partition assignment.</p> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of TopicPartition objects currently assigned to this consumer.</p> <p>Raises:</p> Type Description <code>ConsumerError</code> <p>If retrieving the assignment fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; partitions = consumer.assignment()\n&gt;&gt;&gt; for tp in partitions:\n...     print(f\"{tp.topic} [{tp.partition}]\")\n</code></pre>"},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer.close","title":"<code>close()</code>","text":"<p>Close the consumer and leave the consumer group.</p> <p>It's recommended to use the consumer as a context manager instead of calling this method directly.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; consumer = KafkaConsumer(config)\n&gt;&gt;&gt; try:\n...     consumer.subscribe([\"topic\"])\n...     for msg in consumer:\n...         process(msg)\n... finally:\n...     consumer.close()\n</code></pre> <pre><code>&gt;&gt;&gt; # Better: use context manager\n&gt;&gt;&gt; with KafkaConsumer(config) as consumer:\n...     consumer.subscribe([\"topic\"])\n...     for msg in consumer:\n...         process(msg)\n</code></pre>"},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer.commit","title":"<code>commit(message=None, asynchronous=True)</code>","text":"<p>Commit offsets to Kafka.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Optional[KafkaMessage]</code> <p>Specific message to commit. If None, commits all consumed messages.</p> <code>None</code> <code>asynchronous</code> <code>bool</code> <p>If True, commit asynchronously (default). If False, wait for confirmation.</p> <code>True</code> <p>Raises:</p> Type Description <code>ConsumerError</code> <p>If commit fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Commit after processing each message\n&gt;&gt;&gt; msg = consumer.poll()\n&gt;&gt;&gt; if msg:\n...     process(msg)\n...     consumer.commit(msg)\n</code></pre> <pre><code>&gt;&gt;&gt; # Commit all consumed messages\n&gt;&gt;&gt; consumer.commit()\n</code></pre> <pre><code>&gt;&gt;&gt; # Synchronous commit (wait for confirmation)\n&gt;&gt;&gt; consumer.commit(msg, asynchronous=False)\n</code></pre>"},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer.poll","title":"<code>poll(timeout=1.0)</code>","text":"<p>Poll for a single message.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum time to wait for a message in seconds (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Optional[KafkaMessage]</code> <p>KafkaMessage if a message was received, None if timeout expired</p> <p>Raises:</p> Type Description <code>ConsumerError</code> <p>If an error occurs during polling</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Poll with default 1 second timeout\n&gt;&gt;&gt; msg = consumer.poll()\n&gt;&gt;&gt; if msg:\n...     print(f\"Received: {msg.value_as_string()}\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Poll with longer timeout\n&gt;&gt;&gt; msg = consumer.poll(timeout=5.0)\n</code></pre> <pre><code>&gt;&gt;&gt; # Poll in a loop\n&gt;&gt;&gt; while True:\n...     msg = consumer.poll(timeout=1.0)\n...     if msg:\n...         process(msg)\n...         consumer.commit(msg)\n</code></pre>"},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer.poll_batch","title":"<code>poll_batch(max_messages=100, timeout=1.0)</code>","text":"<p>Poll for a batch of messages.</p> <p>Parameters:</p> Name Type Description Default <code>max_messages</code> <code>int</code> <p>Maximum number of messages to return (default: 100)</p> <code>100</code> <code>timeout</code> <code>float</code> <p>Maximum time to wait in seconds (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>list[KafkaMessage]</code> <p>List of KafkaMessage objects (may be empty if timeout expires)</p> <p>Raises:</p> Type Description <code>ConsumerError</code> <p>If an error occurs during polling</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; messages = consumer.poll_batch(max_messages=50, timeout=2.0)\n&gt;&gt;&gt; for msg in messages:\n...     process(msg)\n</code></pre>"},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer.position","title":"<code>position(partitions)</code>","text":"<p>Get the current position (offset) for the given partitions.</p> <p>Parameters:</p> Name Type Description Default <code>partitions</code> <code>list[Any]</code> <p>List of TopicPartition objects to query.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of TopicPartition objects with offset set to the current position.</p> <p>Raises:</p> Type Description <code>ConsumerError</code> <p>If retrieving the position fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from confluent_kafka import TopicPartition\n&gt;&gt;&gt; positions = consumer.position([TopicPartition(\"my-topic\", 0)])\n&gt;&gt;&gt; for tp in positions:\n...     print(f\"Offset: {tp.offset}\")\n</code></pre>"},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer.seek","title":"<code>seek(partition)</code>","text":"<p>Seek to a specific offset on a partition.</p> <p>Parameters:</p> Name Type Description Default <code>partition</code> <code>Any</code> <p>A TopicPartition object with topic, partition, and offset set.</p> required <p>Raises:</p> Type Description <code>ConsumerError</code> <p>If the seek fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from confluent_kafka import TopicPartition\n&gt;&gt;&gt; consumer.seek(TopicPartition(\"my-topic\", 0, 100))\n</code></pre>"},{"location":"api/consumer/#typedkafka.consumer.KafkaConsumer.subscribe","title":"<code>subscribe(topics, on_assign=None, on_revoke=None, on_lost=None)</code>","text":"<p>Subscribe to one or more topics.</p> <p>Parameters:</p> Name Type Description Default <code>topics</code> <code>list[str]</code> <p>List of topic names to subscribe to</p> required <code>on_assign</code> <code>Optional[Callable[[Any, Any], None]]</code> <p>Callback invoked when partitions are assigned. Signature: callback(consumer, partitions)</p> <code>None</code> <code>on_revoke</code> <code>Optional[Callable[[Any, Any], None]]</code> <p>Callback invoked when partitions are revoked. Signature: callback(consumer, partitions)</p> <code>None</code> <code>on_lost</code> <code>Optional[Callable[[Any, Any], None]]</code> <p>Callback invoked when partitions are lost (unclean). Signature: callback(consumer, partitions)</p> <code>None</code> <p>Raises:</p> Type Description <code>ConsumerError</code> <p>If subscription fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Subscribe to a single topic\n&gt;&gt;&gt; consumer.subscribe([\"my-topic\"])\n</code></pre> <pre><code>&gt;&gt;&gt; # Subscribe to multiple topics\n&gt;&gt;&gt; consumer.subscribe([\"orders\", \"payments\", \"shipments\"])\n</code></pre> <pre><code>&gt;&gt;&gt; # Subscribe with rebalance callbacks\n&gt;&gt;&gt; def on_assign(consumer, partitions):\n...     print(f\"Assigned: {partitions}\")\n&gt;&gt;&gt; def on_revoke(consumer, partitions):\n...     print(f\"Revoked: {partitions}\")\n&gt;&gt;&gt; consumer.subscribe([\"my-topic\"], on_assign=on_assign, on_revoke=on_revoke)\n</code></pre>"},{"location":"api/consumer/#typedkafka.consumer.KafkaMessage","title":"<code>typedkafka.consumer.KafkaMessage</code>","text":"<p>A Kafka message with convenient access methods.</p> <p>Wraps confluent-kafka's Message with better documentation and helper methods.</p> <p>Attributes:</p> Name Type Description <code>topic</code> <p>The topic this message came from</p> <code>partition</code> <p>The partition number</p> <code>offset</code> <p>The message offset</p> <code>key</code> <p>The message key as bytes (None if no key)</p> <code>value</code> <p>The message value as bytes</p> <code>timestamp</code> <p>Message timestamp (type, value) tuple</p> <code>headers</code> <p>Message headers as list of (key, value) tuples</p>"},{"location":"api/consumer/#typedkafka.consumer.KafkaMessage.__init__","title":"<code>__init__(message)</code>","text":"<p>Initialize from a confluent-kafka Message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Any</code> <p>A confluent_kafka.Message object</p> required"},{"location":"api/consumer/#typedkafka.consumer.KafkaMessage.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of the message.</p>"},{"location":"api/consumer/#typedkafka.consumer.KafkaMessage.key_as_string","title":"<code>key_as_string(encoding='utf-8')</code>","text":"<p>Decode the message key as a UTF-8 string.</p> <p>Parameters:</p> Name Type Description Default <code>encoding</code> <code>str</code> <p>Character encoding to use (default: utf-8)</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Decoded string key, or None if no key</p> <p>Raises:</p> Type Description <code>SerializationError</code> <p>If decoding fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; msg = consumer.poll()\n&gt;&gt;&gt; if msg.key_as_string():\n...     print(f\"Key: {msg.key_as_string()}\")\n</code></pre>"},{"location":"api/consumer/#typedkafka.consumer.KafkaMessage.value_as","title":"<code>value_as(deserializer)</code>","text":"<p>Decode the message value using a custom deserializer function.</p> <p>Parameters:</p> Name Type Description Default <code>deserializer</code> <code>Callable[[bytes], T]</code> <p>A callable that takes bytes and returns the desired type.</p> required <p>Returns:</p> Type Description <code>T</code> <p>Deserialized value</p> <p>Raises:</p> Type Description <code>SerializationError</code> <p>If deserialization fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from dataclasses import dataclass\n&gt;&gt;&gt; @dataclass\n... class UserEvent:\n...     user_id: int\n...     @classmethod\n...     def from_bytes(cls, data: bytes) -&gt; \"UserEvent\":\n...         d = json.loads(data)\n...         return cls(user_id=d[\"user_id\"])\n&gt;&gt;&gt; event = msg.value_as(UserEvent.from_bytes)\n</code></pre>"},{"location":"api/consumer/#typedkafka.consumer.KafkaMessage.value_as_json","title":"<code>value_as_json()</code>","text":"<p>Deserialize the message value as JSON.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Parsed JSON object (dict, list, str, int, etc.)</p> <p>Raises:</p> Type Description <code>SerializationError</code> <p>If JSON parsing fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; msg = consumer.poll()\n&gt;&gt;&gt; data = msg.value_as_json()\n&gt;&gt;&gt; print(f\"User ID: {data['user_id']}\")\n</code></pre>"},{"location":"api/consumer/#typedkafka.consumer.KafkaMessage.value_as_string","title":"<code>value_as_string(encoding='utf-8')</code>","text":"<p>Decode the message value as a UTF-8 string.</p> <p>Parameters:</p> Name Type Description Default <code>encoding</code> <code>str</code> <p>Character encoding to use (default: utf-8)</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>str</code> <p>Decoded string value</p> <p>Raises:</p> Type Description <code>SerializationError</code> <p>If decoding fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; msg = consumer.poll()\n&gt;&gt;&gt; text = msg.value_as_string()\n&gt;&gt;&gt; print(f\"Received: {text}\")\n</code></pre>"},{"location":"api/dlq/","title":"Dead Letter Queue","text":""},{"location":"api/dlq/#typedkafka.dlq.DeadLetterQueue","title":"<code>typedkafka.dlq.DeadLetterQueue</code>","text":"<p>Routes failed messages to a dead letter topic.</p> <p>Wraps an existing producer (KafkaProducer or MockProducer) and sends failed messages to a DLQ topic with error metadata in headers.</p> <p>Parameters:</p> Name Type Description Default <code>producer</code> <code>Any</code> <p>A KafkaProducer or MockProducer instance used to send DLQ messages.</p> required <code>topic_fn</code> <code>Optional[Callable[[str], str]]</code> <p>Optional callable that maps original topic to DLQ topic name. Default: appends \".dlq\" to the original topic.</p> <code>None</code> <code>default_topic</code> <code>Optional[str]</code> <p>Optional fixed DLQ topic name. If set, all messages go here regardless of original topic. Mutually exclusive with topic_fn.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both topic_fn and default_topic are provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from typedkafka import KafkaProducer\n&gt;&gt;&gt; producer = KafkaProducer({\"bootstrap.servers\": \"localhost:9092\"})\n&gt;&gt;&gt; dlq = DeadLetterQueue(producer)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Route a failed message\n&gt;&gt;&gt; dlq.send(message, error=exc)\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom topic naming\n&gt;&gt;&gt; dlq = DeadLetterQueue(producer, topic_fn=lambda t: f\"errors.{t}\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Fixed DLQ topic\n&gt;&gt;&gt; dlq = DeadLetterQueue(producer, default_topic=\"all-errors\")\n</code></pre>"},{"location":"api/dlq/#typedkafka.dlq.DeadLetterQueue.send_count","title":"<code>send_count</code>  <code>property</code>","text":"<p>Number of messages sent to the DLQ.</p>"},{"location":"api/dlq/#typedkafka.dlq.DeadLetterQueue.send","title":"<code>send(message, error=None, extra_headers=None)</code>","text":"<p>Send a failed message to the dead letter topic.</p> <p>Preserves the original message value and key, and adds error metadata as Kafka headers.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Any</code> <p>A KafkaMessage or MockMessage that failed processing.</p> required <code>error</code> <code>Optional[Exception]</code> <p>The exception that caused the failure (optional).</p> <code>None</code> <code>extra_headers</code> <code>Optional[list[tuple[str, bytes]]]</code> <p>Additional headers to include.</p> <code>None</code> <p>Raises:</p> Type Description <code>ProducerError</code> <p>If sending to the DLQ topic fails.</p> Headers added automatically <ul> <li>dlq.original.topic: Original topic name</li> <li>dlq.original.partition: Original partition number</li> <li>dlq.original.offset: Original message offset</li> <li>dlq.timestamp: Unix timestamp when DLQ send occurred</li> <li>dlq.error.message: Error string (if error provided)</li> <li>dlq.error.type: Error class name (if error provided)</li> <li>dlq.error.traceback: Full traceback (if error provided)</li> </ul>"},{"location":"api/dlq/#typedkafka.dlq.process_with_dlq","title":"<code>typedkafka.dlq.process_with_dlq(message, handler, dlq)</code>","text":"<p>Process a message, routing to DLQ on failure.</p> <p>Calls the handler with the message. If the handler raises an exception, the message is sent to the dead letter queue with the error details.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Any</code> <p>A KafkaMessage or MockMessage to process.</p> required <code>handler</code> <code>Callable[[Any], None]</code> <p>Callable that processes the message. Should raise on failure.</p> required <code>dlq</code> <code>DeadLetterQueue</code> <p>DeadLetterQueue instance to route failures to.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if processing succeeded, False if message was sent to DLQ.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dlq = DeadLetterQueue(producer)\n&gt;&gt;&gt; for msg in consumer:\n...     success = process_with_dlq(msg, my_handler, dlq)\n...     if success:\n...         consumer.commit(msg)\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#typedkafka.exceptions.KafkaError","title":"<code>typedkafka.exceptions.KafkaError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all Kafka-related errors.</p> <p>All typedkafka exceptions inherit from this base class, making it easy to catch all Kafka-related errors with a single except clause.</p> <p>Attributes:</p> Name Type Description <code>context</code> <p>Structured error context with topic/partition/offset metadata</p> <code>original_error</code> <p>The underlying error that caused this exception</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     producer.send(\"topic\", \"message\")\n... except KafkaError as e:\n...     logger.error(f\"Kafka operation failed: {e}\")\n...     if e.context.topic:\n...         logger.error(f\"Topic: {e.context.topic}\")\n</code></pre>"},{"location":"api/exceptions/#typedkafka.exceptions.KafkaError.__init__","title":"<code>__init__(message, context=None, original_error=None)</code>","text":"<p>Initialize a KafkaError.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Human-readable error description</p> required <code>context</code> <code>Optional[KafkaErrorContext]</code> <p>Structured context about the failed operation</p> <code>None</code> <code>original_error</code> <code>Optional[Exception]</code> <p>The underlying exception that caused this error</p> <code>None</code>"},{"location":"api/exceptions/#typedkafka.exceptions.ProducerError","title":"<code>typedkafka.exceptions.ProducerError</code>","text":"<p>               Bases: <code>KafkaError</code></p> <p>Raised when a Producer operation fails.</p> <p>This exception is raised when message production fails, such as: - Message serialization errors - Network connectivity issues - Broker unavailability - Invalid topic names - Queue full errors</p> <p>Attributes:</p> Name Type Description <code>message</code> <p>Human-readable error description</p> <code>original_error</code> <p>The underlying error from confluent-kafka (if any)</p> <code>context</code> <p>Structured error context</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     producer.send(\"invalid-topic!\", {\"key\": \"value\"})\n... except ProducerError as e:\n...     logger.error(f\"Failed to produce message: {e}\")\n</code></pre>"},{"location":"api/exceptions/#typedkafka.exceptions.ProducerError.__init__","title":"<code>__init__(message, context=None, original_error=None)</code>","text":"<p>Initialize a ProducerError.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Human-readable error description</p> required <code>context</code> <code>Optional[KafkaErrorContext]</code> <p>Structured error context</p> <code>None</code> <code>original_error</code> <code>Optional[Exception]</code> <p>The underlying exception that caused this error</p> <code>None</code>"},{"location":"api/exceptions/#typedkafka.exceptions.ConsumerError","title":"<code>typedkafka.exceptions.ConsumerError</code>","text":"<p>               Bases: <code>KafkaError</code></p> <p>Raised when a Consumer operation fails.</p> <p>This exception is raised when message consumption fails, such as: - Message deserialization errors - Consumer group coordination failures - Offset commit errors - Network connectivity issues</p> <p>Attributes:</p> Name Type Description <code>message</code> <p>Human-readable error description</p> <code>original_error</code> <p>The underlying error from confluent-kafka (if any)</p> <code>context</code> <p>Structured error context</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     for message in consumer:\n...         process(message)\n... except ConsumerError as e:\n...     logger.error(f\"Consumer error: {e}\")\n</code></pre>"},{"location":"api/exceptions/#typedkafka.exceptions.ConsumerError.__init__","title":"<code>__init__(message, context=None, original_error=None)</code>","text":"<p>Initialize a ConsumerError.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Human-readable error description</p> required <code>context</code> <code>Optional[KafkaErrorContext]</code> <p>Structured error context</p> <code>None</code> <code>original_error</code> <code>Optional[Exception]</code> <p>The underlying exception that caused this error</p> <code>None</code>"},{"location":"api/exceptions/#typedkafka.exceptions.SerializationError","title":"<code>typedkafka.exceptions.SerializationError</code>","text":"<p>               Bases: <code>KafkaError</code></p> <p>Raised when message serialization or deserialization fails.</p> <p>This occurs when: - JSON encoding/decoding fails - Avro schema validation fails - Custom serializer raises an exception - Message format is invalid</p> <p>Attributes:</p> Name Type Description <code>message</code> <p>Human-readable error description</p> <code>value</code> <p>The value that failed to serialize/deserialize</p> <code>original_error</code> <p>The underlying error (if any)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     producer.send_json(\"topic\", non_serializable_object)\n... except SerializationError as e:\n...     logger.error(f\"Failed to serialize message: {e}\")\n</code></pre>"},{"location":"api/exceptions/#typedkafka.exceptions.SerializationError.__init__","title":"<code>__init__(message, value=None, context=None, original_error=None)</code>","text":"<p>Initialize a SerializationError.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Human-readable error description</p> required <code>value</code> <code>Any</code> <p>The value that failed to serialize/deserialize</p> <code>None</code> <code>context</code> <code>Optional[KafkaErrorContext]</code> <p>Structured error context</p> <code>None</code> <code>original_error</code> <code>Optional[Exception]</code> <p>The underlying exception that caused this error</p> <code>None</code>"},{"location":"api/metrics/","title":"Metrics","text":""},{"location":"api/metrics/#typedkafka.metrics.KafkaStats","title":"<code>typedkafka.metrics.KafkaStats</code>  <code>dataclass</code>","text":"<p>Parsed Kafka statistics from confluent-kafka's stats_cb callback.</p> <p>This is a subset of the most useful fields from the full statistics JSON. Access the complete data via the <code>raw</code> attribute.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Client name/id</p> <code>client_type</code> <code>str</code> <p>Client type (\"producer\" or \"consumer\")</p> <code>ts</code> <code>int</code> <p>Timestamp in microseconds since epoch</p> <code>time_seconds</code> <code>int</code> <p>Wall clock time in seconds since epoch</p> <code>replyq</code> <code>int</code> <p>Number of operations waiting in queue</p> <code>msg_cnt</code> <code>int</code> <p>Current number of messages in producer queues</p> <code>msg_size</code> <code>int</code> <p>Current total size of messages in producer queues (bytes)</p> <code>tx</code> <code>int</code> <p>Total number of requests sent to brokers</p> <code>rx</code> <code>int</code> <p>Total number of responses received from brokers</p> <code>txbytes</code> <code>int</code> <p>Total number of bytes transmitted to brokers</p> <code>rxbytes</code> <code>int</code> <p>Total number of bytes received from brokers</p> <code>raw</code> <code>dict[str, Any]</code> <p>The full raw statistics dict for advanced use</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; stats = KafkaStats.from_json(json_string)\n&gt;&gt;&gt; print(f\"Bytes sent: {stats.txbytes}\")\n&gt;&gt;&gt; print(f\"Messages queued: {stats.msg_cnt}\")\n</code></pre>"},{"location":"api/metrics/#typedkafka.metrics.KafkaStats.from_json","title":"<code>from_json(json_str)</code>  <code>classmethod</code>","text":"<p>Parse a KafkaStats from the JSON string provided by stats_cb.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>JSON string from confluent-kafka's statistics callback.</p> required <p>Returns:</p> Type Description <code>KafkaStats</code> <p>Parsed KafkaStats instance.</p>"},{"location":"api/metrics/#typedkafka.metrics.KafkaMetrics","title":"<code>typedkafka.metrics.KafkaMetrics</code>  <code>dataclass</code>","text":"<p>Simple counters tracked by the producer or consumer.</p> <p>Updated automatically during normal operations (send, poll). When statistics reporting is enabled via <code>statistics.interval.ms</code>, byte counters and <code>last_stats</code> are also populated.</p> <p>Attributes:</p> Name Type Description <code>messages_sent</code> <code>int</code> <p>Total messages successfully queued (producer only)</p> <code>messages_received</code> <code>int</code> <p>Total messages received (consumer only)</p> <code>errors</code> <code>int</code> <p>Total error count</p> <code>bytes_sent</code> <code>int</code> <p>Total bytes transmitted (from stats callback)</p> <code>bytes_received</code> <code>int</code> <p>Total bytes received (from stats callback)</p> <code>last_stats</code> <code>Optional[KafkaStats]</code> <p>Most recent KafkaStats snapshot (None if stats not enabled)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; producer = KafkaProducer(config)\n&gt;&gt;&gt; producer.send(\"topic\", b\"hello\")\n&gt;&gt;&gt; print(producer.metrics.messages_sent)  # 1\n</code></pre>"},{"location":"api/metrics/#typedkafka.metrics.KafkaMetrics.reset","title":"<code>reset()</code>","text":"<p>Reset all counters to zero.</p>"},{"location":"api/producer/","title":"Producer","text":""},{"location":"api/producer/#typedkafka.producer.KafkaProducer","title":"<code>typedkafka.producer.KafkaProducer</code>","text":"<p>A well-documented Kafka producer with full type hints.</p> <p>This class wraps confluent-kafka's Producer with: - Comprehensive docstrings on every method - Full type hints for IDE autocomplete - Better error messages - Convenient methods for common operations (send_json, send_string) - Context manager support for automatic cleanup</p> Basic Usage <p>producer = KafkaProducer({\"bootstrap.servers\": \"localhost:9092\"}) producer.send(\"my-topic\", b\"my message\", key=b\"my-key\") producer.flush()  # Wait for all messages to be delivered</p> With Context Manager <p>with KafkaProducer({\"bootstrap.servers\": \"localhost:9092\"}) as producer: ...     producer.send(\"my-topic\", b\"message\") ...     # Automatic flush and cleanup on exit</p> JSON Messages <p>producer.send_json(\"events\", {\"user_id\": 123, \"action\": \"click\"})</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>The configuration dictionary used to initialize the producer</p>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.metrics","title":"<code>metrics</code>  <code>property</code>","text":"<p>Current metrics for this producer.</p> <p>Tracks messages sent, errors, and (if stats enabled) byte throughput.</p>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter context manager.</p> <p>Returns:</p> Type Description <code>KafkaProducer</code> <p>self</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with KafkaProducer({\"bootstrap.servers\": \"localhost:9092\"}) as producer:\n...     producer.send(\"topic\", b\"message\")\n</code></pre>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Exit context manager and cleanup resources.</p> <p>Automatically flushes all pending messages before exiting.</p>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.__init__","title":"<code>__init__(config, on_stats=None)</code>","text":"<p>Initialize a Kafka producer with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary for the producer. Common options: - bootstrap.servers (str): Comma-separated list of broker addresses   Example: \"localhost:9092\" or \"broker1:9092,broker2:9092\" - client.id (str): An identifier for this client - acks (str|int): Number of acknowledgments the producer requires   \"0\" = no acknowledgment, \"1\" = leader only, \"all\" = all replicas - compression.type (str): Compression codec (\"none\", \"gzip\", \"snappy\", \"lz4\", \"zstd\") - max.in.flight.requests.per.connection (int): Max unacknowledged requests - linger.ms (int): Time to wait before sending a batch - batch.size (int): Maximum size of a message batch in bytes - statistics.interval.ms (int): Stats reporting interval in milliseconds</p> required <code>on_stats</code> <code>Optional[StatsCallback]</code> <p>Optional callback receiving parsed KafkaStats each reporting interval. Requires <code>statistics.interval.ms</code> to be set in config.</p> <code>None</code> <p>Raises:</p> Type Description <code>ProducerError</code> <p>If the producer cannot be initialized with the given config</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic producer\n&gt;&gt;&gt; producer = KafkaProducer({\"bootstrap.servers\": \"localhost:9092\"})\n</code></pre> <pre><code>&gt;&gt;&gt; # Producer with metrics\n&gt;&gt;&gt; producer = KafkaProducer({\n...     \"bootstrap.servers\": \"localhost:9092\",\n...     \"statistics.interval.ms\": 5000,\n... })\n&gt;&gt;&gt; print(producer.metrics.messages_sent)\n</code></pre>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.abort_transaction","title":"<code>abort_transaction(timeout=30.0)</code>","text":"<p>Abort the current transaction.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum time to wait for abort in seconds.</p> <code>30.0</code> <p>Raises:</p> Type Description <code>TransactionError</code> <p>If aborting the transaction fails.</p>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.begin_transaction","title":"<code>begin_transaction()</code>","text":"<p>Begin a new transaction.</p> <p>Raises:</p> Type Description <code>TransactionError</code> <p>If beginning the transaction fails.</p>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.close","title":"<code>close()</code>","text":"<p>Close the producer and release resources.</p> <p>Calls flush() to ensure all queued messages are delivered before closing. It's recommended to use the producer as a context manager instead of calling this method directly.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; producer = KafkaProducer({\"bootstrap.servers\": \"localhost:9092\"})\n&gt;&gt;&gt; try:\n...     producer.send(\"topic\", b\"message\")\n... finally:\n...     producer.close()  # Ensure cleanup\n</code></pre> <pre><code>&gt;&gt;&gt; # Better: use context manager\n&gt;&gt;&gt; with KafkaProducer({\"bootstrap.servers\": \"localhost:9092\"}) as producer:\n...     producer.send(\"topic\", b\"message\")\n</code></pre>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.commit_transaction","title":"<code>commit_transaction(timeout=30.0)</code>","text":"<p>Commit the current transaction.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum time to wait for commit in seconds.</p> <code>30.0</code> <p>Raises:</p> Type Description <code>TransactionError</code> <p>If committing the transaction fails.</p>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.flush","title":"<code>flush(timeout=-1)</code>","text":"<p>Wait for all messages in the queue to be delivered.</p> <p>Blocks until all messages are sent or the timeout expires.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum time to wait in seconds. Use -1 for infinite wait (default). Example: 5.0 = wait up to 5 seconds</p> <code>-1</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of messages still in queue/internal Producer state. 0 means all delivered.</p> <p>Raises:</p> Type Description <code>ProducerError</code> <p>If flush fails</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Wait for all messages to be delivered\n&gt;&gt;&gt; producer.send(\"topic\", b\"message 1\")\n&gt;&gt;&gt; producer.send(\"topic\", b\"message 2\")\n&gt;&gt;&gt; remaining = producer.flush()\n&gt;&gt;&gt; if remaining == 0:\n...     print(\"All messages delivered successfully\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Wait up to 5 seconds\n&gt;&gt;&gt; remaining = producer.flush(timeout=5.0)\n&gt;&gt;&gt; if remaining &gt; 0:\n...     print(f\"Warning: {remaining} messages not delivered after 5 seconds\")\n</code></pre>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.init_transactions","title":"<code>init_transactions(timeout=30.0)</code>","text":"<p>Initialize the producer for transactions.</p> <p>Must be called before any transactional methods. Requires the <code>transactional.id</code> configuration to be set.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum time to wait for initialization in seconds.</p> <code>30.0</code> <p>Raises:</p> Type Description <code>TransactionError</code> <p>If transaction initialization fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; producer = KafkaProducer({\n...     \"bootstrap.servers\": \"localhost:9092\",\n...     \"transactional.id\": \"my-txn-id\",\n... })\n&gt;&gt;&gt; producer.init_transactions()\n</code></pre>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.send","title":"<code>send(topic, value, key=None, partition=None, on_delivery=None, headers=None)</code>","text":"<p>Send a message to a Kafka topic.</p> <p>This method is asynchronous - it returns immediately after queuing the message. Use flush() to wait for delivery confirmation.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>The topic name to send the message to</p> required <code>value</code> <code>bytes</code> <p>The message payload as bytes</p> required <code>key</code> <code>Optional[bytes]</code> <p>Optional message key as bytes. Messages with the same key go to the same partition.</p> <code>None</code> <code>partition</code> <code>Optional[int]</code> <p>Optional partition number. If None, partition is chosen by the partitioner.</p> <code>None</code> <code>on_delivery</code> <code>Optional[DeliveryCallback]</code> <p>Optional callback function called when delivery succeeds or fails. Signature: callback(error, message)</p> <code>None</code> <code>headers</code> <code>Optional[list[tuple[str, bytes]]]</code> <p>Optional list of (key, value) header tuples to include with the message.</p> <code>None</code> <p>Raises:</p> Type Description <code>ProducerError</code> <p>If the message cannot be queued (e.g., queue is full)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Send a simple message\n&gt;&gt;&gt; producer.send(\"my-topic\", b\"Hello, Kafka!\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Send with a key for partitioning\n&gt;&gt;&gt; producer.send(\"user-events\", b\"event data\", key=b\"user-123\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Send with headers\n&gt;&gt;&gt; producer.send(\"topic\", b\"data\", headers=[(\"trace-id\", b\"abc123\")])\n</code></pre>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.send_batch","title":"<code>send_batch(topic, messages, on_delivery=None)</code>","text":"<p>Send a batch of messages to a Kafka topic.</p> <p>Each message is a tuple of (value, key). This is more efficient than calling send() repeatedly as it defers polling until after all messages are queued.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>The topic name to send the messages to</p> required <code>messages</code> <code>list[tuple[bytes, Optional[bytes]]]</code> <p>List of (value, key) tuples. Key can be None.</p> required <code>on_delivery</code> <code>Optional[DeliveryCallback]</code> <p>Optional callback for each message delivery.</p> <code>None</code> <p>Raises:</p> Type Description <code>ProducerError</code> <p>If any message cannot be queued</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; producer.send_batch(\"events\", [\n...     (b\"event1\", b\"key1\"),\n...     (b\"event2\", b\"key2\"),\n...     (b\"event3\", None),\n... ])\n&gt;&gt;&gt; producer.flush()\n</code></pre>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.send_json","title":"<code>send_json(topic, value, key=None, partition=None, on_delivery=None)</code>","text":"<p>Send a JSON-serialized message to a Kafka topic.</p> <p>Convenience method that automatically serializes Python objects to JSON.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>The topic name to send the message to</p> required <code>value</code> <code>Any</code> <p>Any JSON-serializable Python object (dict, list, str, int, etc.)</p> required <code>key</code> <code>Optional[str]</code> <p>Optional string key (will be UTF-8 encoded)</p> <code>None</code> <code>partition</code> <code>Optional[int]</code> <p>Optional partition number</p> <code>None</code> <code>on_delivery</code> <code>Optional[DeliveryCallback]</code> <p>Optional callback function for delivery confirmation</p> <code>None</code> <p>Raises:</p> Type Description <code>SerializationError</code> <p>If the value cannot be serialized to JSON</p> <code>ProducerError</code> <p>If the message cannot be queued</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Send a dict as JSON\n&gt;&gt;&gt; producer.send_json(\"events\", {\"user_id\": 123, \"action\": \"click\"})\n</code></pre> <pre><code>&gt;&gt;&gt; # Send with a string key\n&gt;&gt;&gt; producer.send_json(\"user-data\", {\"name\": \"Alice\"}, key=\"user-123\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Send a list\n&gt;&gt;&gt; producer.send_json(\"numbers\", [1, 2, 3, 4, 5])\n</code></pre>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.send_string","title":"<code>send_string(topic, value, key=None, partition=None, on_delivery=None)</code>","text":"<p>Send a UTF-8 encoded string message to a Kafka topic.</p> <p>Convenience method for sending text messages.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>The topic name to send the message to</p> required <code>value</code> <code>str</code> <p>String message to send</p> required <code>key</code> <code>Optional[str]</code> <p>Optional string key</p> <code>None</code> <code>partition</code> <code>Optional[int]</code> <p>Optional partition number</p> <code>None</code> <code>on_delivery</code> <code>Optional[DeliveryCallback]</code> <p>Optional callback function for delivery confirmation</p> <code>None</code> <p>Raises:</p> Type Description <code>ProducerError</code> <p>If the message cannot be queued</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; producer.send_string(\"logs\", \"Application started successfully\")\n&gt;&gt;&gt; producer.send_string(\"user-messages\", \"Hello!\", key=\"user-123\")\n</code></pre>"},{"location":"api/producer/#typedkafka.producer.KafkaProducer.transaction","title":"<code>transaction()</code>","text":"<p>Return a context manager for transactional sends.</p> <p>Automatically begins, commits, or aborts the transaction.</p> <p>Returns:</p> Type Description <code>TransactionContext</code> <p>A context manager that manages the transaction lifecycle.</p> <p>Raises:</p> Type Description <code>ProducerError</code> <p>If any transaction operation fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; producer = KafkaProducer({\n...     \"bootstrap.servers\": \"localhost:9092\",\n...     \"transactional.id\": \"my-txn-id\",\n... })\n&gt;&gt;&gt; producer.init_transactions()\n&gt;&gt;&gt; with producer.transaction():\n...     producer.send(\"topic\", b\"msg1\")\n...     producer.send(\"topic\", b\"msg2\")\n</code></pre>"},{"location":"api/producer/#typedkafka.producer.TransactionContext","title":"<code>typedkafka.producer.TransactionContext</code>","text":"<p>Context manager for Kafka transactions.</p> <p>Begins a transaction on entry and commits on clean exit. Aborts the transaction if an exception occurs.</p>"},{"location":"api/retry/","title":"Retry","text":""},{"location":"api/retry/#typedkafka.retry.retry","title":"<code>typedkafka.retry.retry(max_attempts=3, backoff_base=0.5, backoff_max=30.0, jitter=True, retryable_exceptions=None)</code>","text":"<p>Decorator that retries a function on failure with exponential backoff.</p> <p>Parameters:</p> Name Type Description Default <code>max_attempts</code> <code>int</code> <p>Maximum number of attempts (including the first call).</p> <code>3</code> <code>backoff_base</code> <code>float</code> <p>Base delay in seconds for exponential backoff.</p> <code>0.5</code> <code>backoff_max</code> <code>float</code> <p>Maximum delay in seconds between retries.</p> <code>30.0</code> <code>jitter</code> <code>bool</code> <p>If True, add random jitter to backoff delay.</p> <code>True</code> <code>retryable_exceptions</code> <code>Optional[Sequence[type[BaseException]]]</code> <p>Exception types to retry on. Defaults to <code>(KafkaError,)</code> if not specified.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[F], F]</code> <p>Decorated function that retries on failure.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from typedkafka.retry import retry\n&gt;&gt;&gt; from typedkafka.exceptions import ProducerError\n&gt;&gt;&gt;\n&gt;&gt;&gt; @retry(max_attempts=3, backoff_base=1.0)\n... def send_message(producer, topic, value):\n...     producer.send(topic, value)\n...     producer.flush()\n</code></pre> <pre><code>&gt;&gt;&gt; # Retry only specific exceptions\n&gt;&gt;&gt; @retry(max_attempts=5, retryable_exceptions=(ProducerError,))\n... def produce(producer, data):\n...     producer.send_json(\"events\", data)\n</code></pre>"},{"location":"api/retry/#typedkafka.retry.RetryPolicy","title":"<code>typedkafka.retry.RetryPolicy</code>","text":"<p>Configurable retry policy for Kafka operations.</p> <p>Provides a reusable retry configuration that can be applied to multiple operations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; policy = RetryPolicy(max_attempts=5, backoff_base=1.0)\n&gt;&gt;&gt; result = policy.execute(lambda: producer.send(\"topic\", b\"msg\"))\n</code></pre>"},{"location":"api/retry/#typedkafka.retry.RetryPolicy.__init__","title":"<code>__init__(max_attempts=3, backoff_base=0.5, backoff_max=30.0, jitter=True, retryable_exceptions=None)</code>","text":"<p>Initialize a retry policy.</p> <p>Parameters:</p> Name Type Description Default <code>max_attempts</code> <code>int</code> <p>Maximum number of attempts.</p> <code>3</code> <code>backoff_base</code> <code>float</code> <p>Base delay in seconds for exponential backoff.</p> <code>0.5</code> <code>backoff_max</code> <code>float</code> <p>Maximum delay between retries.</p> <code>30.0</code> <code>jitter</code> <code>bool</code> <p>If True, add random jitter to delays.</p> <code>True</code> <code>retryable_exceptions</code> <code>Optional[Sequence[type[BaseException]]]</code> <p>Exception types to retry on. Defaults to <code>(KafkaError,)</code>.</p> <code>None</code>"},{"location":"api/retry/#typedkafka.retry.RetryPolicy.execute","title":"<code>execute(func, *args, **kwargs)</code>","text":"<p>Execute a function with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The function to execute.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The return value of the function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; policy = RetryPolicy(max_attempts=3)\n&gt;&gt;&gt; policy.execute(producer.send, \"topic\", b\"value\")\n</code></pre>"},{"location":"api/serializers/","title":"Serializers","text":""},{"location":"api/serializers/#typedkafka.serializers.Serializer","title":"<code>typedkafka.serializers.Serializer</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Abstract base class for message serializers.</p> <p>Implement this interface to create custom serializers for use with KafkaProducer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MySerializer(Serializer[dict]):\n...     def serialize(self, topic, value):\n...         return json.dumps(value).encode()\n</code></pre>"},{"location":"api/serializers/#typedkafka.serializers.Serializer.serialize","title":"<code>serialize(topic, value)</code>  <code>abstractmethod</code>","text":"<p>Serialize a value to bytes.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>The topic the message will be sent to.</p> required <code>value</code> <code>T</code> <p>The value to serialize.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>Serialized bytes.</p>"},{"location":"api/serializers/#typedkafka.serializers.Deserializer","title":"<code>typedkafka.serializers.Deserializer</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Abstract base class for message deserializers.</p> <p>Implement this interface to create custom deserializers for use with KafkaConsumer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyDeserializer(Deserializer[dict]):\n...     def deserialize(self, topic, data):\n...         return json.loads(data.decode())\n</code></pre>"},{"location":"api/serializers/#typedkafka.serializers.Deserializer.deserialize","title":"<code>deserialize(topic, data)</code>  <code>abstractmethod</code>","text":"<p>Deserialize bytes to a value.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>The topic the message came from.</p> required <code>data</code> <code>bytes</code> <p>Raw bytes to deserialize.</p> required <p>Returns:</p> Type Description <code>T</code> <p>Deserialized value.</p>"},{"location":"api/serializers/#typedkafka.serializers.JsonSerializer","title":"<code>typedkafka.serializers.JsonSerializer</code>","text":"<p>               Bases: <code>Serializer[Any]</code></p> <p>JSON serializer that encodes Python objects to UTF-8 JSON bytes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ser = JsonSerializer()\n&gt;&gt;&gt; ser.serialize(\"topic\", {\"user_id\": 123})\nb'{\"user_id\": 123}'\n</code></pre>"},{"location":"api/serializers/#typedkafka.serializers.JsonSerializer.serialize","title":"<code>serialize(topic, value)</code>","text":"<p>Serialize a value to JSON bytes.</p>"},{"location":"api/serializers/#typedkafka.serializers.JsonDeserializer","title":"<code>typedkafka.serializers.JsonDeserializer</code>","text":"<p>               Bases: <code>Deserializer[Any]</code></p> <p>JSON deserializer that decodes UTF-8 JSON bytes to Python objects.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; deser = JsonDeserializer()\n&gt;&gt;&gt; deser.deserialize(\"topic\", b'{\"user_id\": 123}')\n{'user_id': 123}\n</code></pre>"},{"location":"api/serializers/#typedkafka.serializers.JsonDeserializer.deserialize","title":"<code>deserialize(topic, data)</code>","text":"<p>Deserialize JSON bytes to a Python object.</p>"},{"location":"api/serializers/#typedkafka.serializers.StringSerializer","title":"<code>typedkafka.serializers.StringSerializer</code>","text":"<p>               Bases: <code>Serializer[str]</code></p> <p>String serializer that encodes strings to bytes.</p> <p>Parameters:</p> Name Type Description Default <code>encoding</code> <code>str</code> <p>Character encoding to use (default: utf-8).</p> <code>'utf-8'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ser = StringSerializer()\n&gt;&gt;&gt; ser.serialize(\"topic\", \"hello\")\nb'hello'\n</code></pre>"},{"location":"api/serializers/#typedkafka.serializers.StringSerializer.serialize","title":"<code>serialize(topic, value)</code>","text":"<p>Serialize a string to bytes.</p>"},{"location":"api/serializers/#typedkafka.serializers.StringDeserializer","title":"<code>typedkafka.serializers.StringDeserializer</code>","text":"<p>               Bases: <code>Deserializer[str]</code></p> <p>String deserializer that decodes bytes to strings.</p> <p>Parameters:</p> Name Type Description Default <code>encoding</code> <code>str</code> <p>Character encoding to use (default: utf-8).</p> <code>'utf-8'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; deser = StringDeserializer()\n&gt;&gt;&gt; deser.deserialize(\"topic\", b\"hello\")\n'hello'\n</code></pre>"},{"location":"api/serializers/#typedkafka.serializers.StringDeserializer.deserialize","title":"<code>deserialize(topic, data)</code>","text":"<p>Deserialize bytes to a string.</p>"},{"location":"api/testing/","title":"Testing","text":""},{"location":"api/testing/#typedkafka.testing.MockProducer","title":"<code>typedkafka.testing.MockProducer</code>","text":"<p>A mock Kafka producer for testing.</p> <p>Records all messages sent to topics without actually sending to Kafka. Perfect for unit tests to verify your code sends the right messages.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; producer = MockProducer()\n&gt;&gt;&gt; producer.send(\"my-topic\", b\"test message\", key=b\"test-key\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Verify the message was sent\n&gt;&gt;&gt; assert len(producer.messages[\"my-topic\"]) == 1\n&gt;&gt;&gt; msg = producer.messages[\"my-topic\"][0]\n&gt;&gt;&gt; assert msg.value == b\"test message\"\n&gt;&gt;&gt; assert msg.key == b\"test-key\"\n</code></pre> <p>Attributes:</p> Name Type Description <code>messages</code> <code>dict[str, list[MockMessage]]</code> <p>Dict mapping topic names to lists of MockMessage objects</p> <code>call_count</code> <p>Number of times send() was called</p> <code>flushed</code> <p>Whether flush() has been called</p>"},{"location":"api/testing/#typedkafka.testing.MockProducer.metrics","title":"<code>metrics</code>  <code>property</code>","text":"<p>Current metrics for this producer.</p>"},{"location":"api/testing/#typedkafka.testing.MockProducer.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p>"},{"location":"api/testing/#typedkafka.testing.MockProducer.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit.</p>"},{"location":"api/testing/#typedkafka.testing.MockProducer.__init__","title":"<code>__init__(config=None, fail_on_topics=None)</code>","text":"<p>Initialize a mock producer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[dict[str, Any]]</code> <p>Optional config dict (ignored, but accepted for compatibility)</p> <code>None</code> <code>fail_on_topics</code> <code>Optional[set[str]]</code> <p>Optional set of topic names that will cause send() to raise ProducerError. Useful for testing error handling paths.</p> <code>None</code>"},{"location":"api/testing/#typedkafka.testing.MockProducer.abort_transaction","title":"<code>abort_transaction(timeout=30.0)</code>","text":"<p>Abort the mock transaction, discarding buffered messages.</p>"},{"location":"api/testing/#typedkafka.testing.MockProducer.begin_transaction","title":"<code>begin_transaction()</code>","text":"<p>Begin a mock transaction.</p>"},{"location":"api/testing/#typedkafka.testing.MockProducer.close","title":"<code>close()</code>","text":"<p>Mark producer as closed.</p>"},{"location":"api/testing/#typedkafka.testing.MockProducer.commit_transaction","title":"<code>commit_transaction(timeout=30.0)</code>","text":"<p>Commit the mock transaction, flushing buffered messages.</p>"},{"location":"api/testing/#typedkafka.testing.MockProducer.flush","title":"<code>flush(timeout=-1)</code>","text":"<p>Mark producer as flushed (no-op in mock).</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Ignored in mock</p> <code>-1</code> <p>Returns:</p> Type Description <code>int</code> <p>0 (always successful in mock)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; producer = MockProducer()\n&gt;&gt;&gt; producer.send(\"topic\", b\"msg\")\n&gt;&gt;&gt; remaining = producer.flush()\n&gt;&gt;&gt; assert remaining == 0\n&gt;&gt;&gt; assert producer.flushed is True\n</code></pre>"},{"location":"api/testing/#typedkafka.testing.MockProducer.get_json_messages","title":"<code>get_json_messages(topic)</code>","text":"<p>Get all messages for a topic deserialized as JSON.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Topic name</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of deserialized JSON values</p>"},{"location":"api/testing/#typedkafka.testing.MockProducer.init_transactions","title":"<code>init_transactions(timeout=30.0)</code>","text":"<p>Initialize transactions (no-op in mock).</p>"},{"location":"api/testing/#typedkafka.testing.MockProducer.message_count","title":"<code>message_count(topic)</code>","text":"<p>Get count of messages sent to a topic.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Topic name</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of messages sent to the topic</p>"},{"location":"api/testing/#typedkafka.testing.MockProducer.reset","title":"<code>reset()</code>","text":"<p>Clear all recorded messages and reset state.</p> <p>Useful for reusing the same mock across multiple test cases.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; producer = MockProducer()\n&gt;&gt;&gt; producer.send(\"topic\", b\"msg1\")\n&gt;&gt;&gt; producer.reset()\n&gt;&gt;&gt; assert len(producer.messages) == 0\n&gt;&gt;&gt; assert producer.call_count == 0\n</code></pre>"},{"location":"api/testing/#typedkafka.testing.MockProducer.send","title":"<code>send(topic, value, key=None, partition=None, on_delivery=None, headers=None)</code>","text":"<p>Record a message send (doesn't actually send to Kafka).</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Topic to send to</p> required <code>value</code> <code>bytes</code> <p>Message value</p> required <code>key</code> <code>Optional[bytes]</code> <p>Optional message key</p> <code>None</code> <code>partition</code> <code>Optional[int]</code> <p>Optional partition (default: 0)</p> <code>None</code> <code>on_delivery</code> <code>Optional[DeliveryCallback]</code> <p>Optional callback (will be called immediately with success)</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; producer = MockProducer()\n&gt;&gt;&gt; producer.send(\"events\", b\"data\", key=b\"key-1\")\n&gt;&gt;&gt; assert len(producer.messages[\"events\"]) == 1\n</code></pre>"},{"location":"api/testing/#typedkafka.testing.MockProducer.send_batch","title":"<code>send_batch(topic, messages, on_delivery=None)</code>","text":"<p>Record a batch of message sends.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Topic to send to</p> required <code>messages</code> <code>list[tuple[bytes, Optional[bytes]]]</code> <p>List of (value, key) tuples</p> required <code>on_delivery</code> <code>Optional[DeliveryCallback]</code> <p>Optional delivery callback</p> <code>None</code>"},{"location":"api/testing/#typedkafka.testing.MockProducer.send_json","title":"<code>send_json(topic, value, key=None, partition=None, on_delivery=None)</code>","text":"<p>Record a JSON message send.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Topic to send to</p> required <code>value</code> <code>Any</code> <p>JSON-serializable value</p> required <code>key</code> <code>Optional[str]</code> <p>Optional string key</p> <code>None</code> <code>partition</code> <code>Optional[int]</code> <p>Optional partition</p> <code>None</code> <code>on_delivery</code> <code>Optional[DeliveryCallback]</code> <p>Optional delivery callback</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; producer = MockProducer()\n&gt;&gt;&gt; producer.send_json(\"events\", {\"user_id\": 123})\n&gt;&gt;&gt; import json\n&gt;&gt;&gt; data = json.loads(producer.messages[\"events\"][0].value)\n&gt;&gt;&gt; assert data[\"user_id\"] == 123\n</code></pre>"},{"location":"api/testing/#typedkafka.testing.MockProducer.send_string","title":"<code>send_string(topic, value, key=None, partition=None, on_delivery=None)</code>","text":"<p>Record a string message send.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Topic to send to</p> required <code>value</code> <code>str</code> <p>String value</p> required <code>key</code> <code>Optional[str]</code> <p>Optional string key</p> <code>None</code> <code>partition</code> <code>Optional[int]</code> <p>Optional partition</p> <code>None</code> <code>on_delivery</code> <code>Optional[DeliveryCallback]</code> <p>Optional delivery callback</p> <code>None</code>"},{"location":"api/testing/#typedkafka.testing.MockProducer.transaction","title":"<code>transaction()</code>","text":"<p>Return a mock transaction context manager.</p>"},{"location":"api/testing/#typedkafka.testing.MockConsumer","title":"<code>typedkafka.testing.MockConsumer</code>","text":"<p>A mock Kafka consumer for testing.</p> <p>Allows you to inject predefined messages for testing code that consumes from Kafka.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; consumer = MockConsumer()\n&gt;&gt;&gt; consumer.add_message(\"my-topic\", b\"test message\", key=b\"test-key\")\n&gt;&gt;&gt; consumer.subscribe([\"my-topic\"])\n&gt;&gt;&gt;\n&gt;&gt;&gt; msg = consumer.poll()\n&gt;&gt;&gt; assert msg.value == b\"test message\"\n&gt;&gt;&gt; assert msg.key == b\"test-key\"\n</code></pre> <p>Attributes:</p> Name Type Description <code>messages</code> <code>list[MockMessage]</code> <p>Queue of MockMessage objects to be consumed</p> <code>subscribed_topics</code> <code>list[str]</code> <p>List of subscribed topics</p> <code>committed_offsets</code> <code>dict[tuple[str, int], int]</code> <p>Dict of committed offsets by topic/partition</p> <code>poll_timeout</code> <code>float</code> <p>Timeout used by iter (matches KafkaConsumer)</p>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.metrics","title":"<code>metrics</code>  <code>property</code>","text":"<p>Current metrics for this consumer.</p>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit.</p>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize a mock consumer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[dict[str, Any]]</code> <p>Optional config dict (ignored, but accepted for compatibility)</p> <code>None</code>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over queued messages.</p> <p>Yields all queued messages then stops. In tests, use poll() in a loop or add all messages before iterating.</p> <p>Yields:</p> Type Description <p>MockMessage objects until queue is exhausted</p>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.add_json_message","title":"<code>add_json_message(topic, value, key=None, partition=0)</code>","text":"<p>Add a JSON message to be consumed.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Topic name</p> required <code>value</code> <code>Any</code> <p>JSON-serializable value</p> required <code>key</code> <code>Optional[str]</code> <p>Optional string key</p> <code>None</code> <code>partition</code> <code>int</code> <p>Partition number</p> <code>0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; consumer = MockConsumer()\n&gt;&gt;&gt; consumer.add_json_message(\"events\", {\"user_id\": 123, \"action\": \"click\"})\n</code></pre>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.add_message","title":"<code>add_message(topic, value, key=None, partition=0, offset=None, headers=None)</code>","text":"<p>Add a message to be consumed.</p> <p>Call this in your tests to inject messages that your code will consume.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Topic name</p> required <code>value</code> <code>bytes</code> <p>Message value</p> required <code>key</code> <code>Optional[bytes]</code> <p>Optional message key</p> <code>None</code> <code>partition</code> <code>int</code> <p>Partition number (default: 0)</p> <code>0</code> <code>offset</code> <code>Optional[int]</code> <p>Message offset (auto-generated if None)</p> <code>None</code> <code>headers</code> <code>Optional[list[tuple[str, bytes]]]</code> <p>Optional message headers</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; consumer = MockConsumer()\n&gt;&gt;&gt; consumer.add_message(\"events\", b'{\"user_id\": 123}')\n&gt;&gt;&gt; consumer.add_message(\"events\", b'{\"user_id\": 456}')\n&gt;&gt;&gt; assert len(consumer.messages) == 2\n</code></pre>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.add_string_message","title":"<code>add_string_message(topic, value, key=None, partition=0)</code>","text":"<p>Add a string message to be consumed.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Topic name</p> required <code>value</code> <code>str</code> <p>String value</p> required <code>key</code> <code>Optional[str]</code> <p>Optional string key</p> <code>None</code> <code>partition</code> <code>int</code> <p>Partition number</p> <code>0</code>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.assign","title":"<code>assign(partitions)</code>","text":"<p>Manually assign partitions.</p>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.assignment","title":"<code>assignment()</code>","text":"<p>Get the current partition assignment.</p>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.close","title":"<code>close()</code>","text":"<p>Mark consumer as closed.</p>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.commit","title":"<code>commit(message=None, asynchronous=True)</code>","text":"<p>Record a commit (doesn't actually commit to Kafka).</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Optional[MockMessage]</code> <p>Message to commit offset for</p> <code>None</code> <code>asynchronous</code> <code>bool</code> <p>Ignored in mock</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; consumer = MockConsumer()\n&gt;&gt;&gt; consumer.add_message(\"topic\", b\"msg\", partition=0, offset=42)\n&gt;&gt;&gt; msg = consumer.poll()\n&gt;&gt;&gt; consumer.commit(msg)\n&gt;&gt;&gt; assert consumer.committed_offsets[(\"topic\", 0)] == 42\n</code></pre>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.poll","title":"<code>poll(timeout=1.0)</code>","text":"<p>Poll for the next message.</p> <p>Returns messages in the order they were added with add_message().</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Ignored in mock</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Optional[MockMessage]</code> <p>Next MockMessage or None if no more messages</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; consumer = MockConsumer()\n&gt;&gt;&gt; consumer.add_message(\"topic\", b\"msg1\")\n&gt;&gt;&gt; consumer.add_message(\"topic\", b\"msg2\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; msg1 = consumer.poll()\n&gt;&gt;&gt; assert msg1.value == b\"msg1\"\n&gt;&gt;&gt; msg2 = consumer.poll()\n&gt;&gt;&gt; assert msg2.value == b\"msg2\"\n&gt;&gt;&gt; msg3 = consumer.poll()\n&gt;&gt;&gt; assert msg3 is None\n</code></pre>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.poll_batch","title":"<code>poll_batch(max_messages=100, timeout=1.0)</code>","text":"<p>Poll for a batch of messages.</p> <p>Parameters:</p> Name Type Description Default <code>max_messages</code> <code>int</code> <p>Maximum number of messages to return</p> <code>100</code> <code>timeout</code> <code>float</code> <p>Ignored in mock</p> <code>1.0</code> <p>Returns:</p> Type Description <code>list[MockMessage]</code> <p>List of MockMessage objects</p>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.position","title":"<code>position(partitions)</code>","text":"<p>Get current position for partitions (returns input unchanged in mock).</p>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.reset","title":"<code>reset()</code>","text":"<p>Clear all messages and reset state.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; consumer = MockConsumer()\n&gt;&gt;&gt; consumer.add_message(\"topic\", b\"msg\")\n&gt;&gt;&gt; consumer.reset()\n&gt;&gt;&gt; assert len(consumer.messages) == 0\n</code></pre>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.seek","title":"<code>seek(partition)</code>","text":"<p>Seek to a specific offset (recorded but not enforced in mock).</p>"},{"location":"api/testing/#typedkafka.testing.MockConsumer.subscribe","title":"<code>subscribe(topics, on_assign=None, on_revoke=None, on_lost=None)</code>","text":"<p>Subscribe to topics (recorded but not enforced in mock).</p> <p>Parameters:</p> Name Type Description Default <code>topics</code> <code>list[str]</code> <p>List of topic names</p> required <code>on_assign</code> <code>Optional[DeliveryCallback]</code> <p>Optional rebalance callback (stored but not called in mock)</p> <code>None</code> <code>on_revoke</code> <code>Optional[DeliveryCallback]</code> <p>Optional rebalance callback (stored but not called in mock)</p> <code>None</code> <code>on_lost</code> <code>Optional[DeliveryCallback]</code> <p>Optional rebalance callback (stored but not called in mock)</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; consumer = MockConsumer()\n&gt;&gt;&gt; consumer.subscribe([\"topic1\", \"topic2\"])\n&gt;&gt;&gt; assert \"topic1\" in consumer.subscribed_topics\n</code></pre>"}]}